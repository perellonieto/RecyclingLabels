Most of the following floats are limited to a precision of 2 decimals

# DATA DESCRIPTION ###

- name = blobs
- Number of features = 2
- Number of classes = 6
- Classes = [0, 1, 2, 3, 4, 5]
- Samples with only weak = 1800
- Samples with true labels = 200

## HOW THE WEAK LABELS ARE GENERATED ###

This dataset only has true labels. In order to create the weak
            labels we designed a mixing matrix with the following steps:

Given the decimal true labels y

- y = [4 0 0 2 1 4]

We create artificially the mixing matrix M

- method = quasi_IPL
- alpha = 0.5
- beta = 0.3

Resulting mixing matrix M

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **1** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.17 |
| **2** | 0.00 | 0.00 | 0.00 | 0.00 | 0.17 | 0.00 |
| **3** | 0.00 | 0.00 | 0.00 | 0.00 | 0.07 | 0.07 |
| **4** | 0.00 | 0.00 | 0.00 | 0.17 | 0.00 | 0.00 |
| **5** | 0.00 | 0.00 | 0.00 | 0.07 | 0.00 | 0.07 |
| **6** | 0.00 | 0.00 | 0.00 | 0.07 | 0.07 | 0.00 |
| **7** | 0.00 | 0.00 | 0.00 | 0.03 | 0.03 | 0.03 |
| **8** | 0.00 | 0.00 | 0.17 | 0.00 | 0.00 | 0.00 |
| **9** | 0.00 | 0.00 | 0.07 | 0.00 | 0.00 | 0.07 |
| **10** | 0.00 | 0.00 | 0.07 | 0.00 | 0.07 | 0.00 |
| **11** | 0.00 | 0.00 | 0.03 | 0.00 | 0.03 | 0.03 |
| **12** | 0.00 | 0.00 | 0.07 | 0.07 | 0.00 | 0.00 |
| **13** | 0.00 | 0.00 | 0.03 | 0.03 | 0.00 | 0.03 |
| **14** | 0.00 | 0.00 | 0.03 | 0.03 | 0.03 | 0.00 |
| **15** | 0.00 | 0.00 | 0.01 | 0.01 | 0.01 | 0.01 |
| **16** | 0.00 | 0.17 | 0.00 | 0.00 | 0.00 | 0.00 |
| **17** | 0.00 | 0.07 | 0.00 | 0.00 | 0.00 | 0.07 |
| **18** | 0.00 | 0.07 | 0.00 | 0.00 | 0.07 | 0.00 |
| **19** | 0.00 | 0.03 | 0.00 | 0.00 | 0.03 | 0.03 |
| **20** | 0.00 | 0.07 | 0.00 | 0.07 | 0.00 | 0.00 |
| **21** | 0.00 | 0.03 | 0.00 | 0.03 | 0.00 | 0.03 |
| **22** | 0.00 | 0.03 | 0.00 | 0.03 | 0.03 | 0.00 |
| **23** | 0.00 | 0.01 | 0.00 | 0.01 | 0.01 | 0.01 |
| **24** | 0.00 | 0.07 | 0.07 | 0.00 | 0.00 | 0.00 |
| **25** | 0.00 | 0.03 | 0.03 | 0.00 | 0.00 | 0.03 |
| **26** | 0.00 | 0.03 | 0.03 | 0.00 | 0.03 | 0.00 |
| **27** | 0.00 | 0.01 | 0.01 | 0.00 | 0.01 | 0.01 |
| **28** | 0.00 | 0.03 | 0.03 | 0.03 | 0.00 | 0.00 |
| **29** | 0.00 | 0.01 | 0.01 | 0.01 | 0.00 | 0.01 |
| **30** | 0.00 | 0.01 | 0.01 | 0.01 | 0.01 | 0.00 |
| **31** | 0.00 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **32** | 0.17 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **33** | 0.07 | 0.00 | 0.00 | 0.00 | 0.00 | 0.07 |
| **34** | 0.07 | 0.00 | 0.00 | 0.00 | 0.07 | 0.00 |
| **35** | 0.03 | 0.00 | 0.00 | 0.00 | 0.03 | 0.03 |
| **36** | 0.07 | 0.00 | 0.00 | 0.07 | 0.00 | 0.00 |
| **37** | 0.03 | 0.00 | 0.00 | 0.03 | 0.00 | 0.03 |
| **38** | 0.03 | 0.00 | 0.00 | 0.03 | 0.03 | 0.00 |
| **39** | 0.01 | 0.00 | 0.00 | 0.01 | 0.01 | 0.01 |
| **40** | 0.07 | 0.00 | 0.07 | 0.00 | 0.00 | 0.00 |
| **41** | 0.03 | 0.00 | 0.03 | 0.00 | 0.00 | 0.03 |
| **42** | 0.03 | 0.00 | 0.03 | 0.00 | 0.03 | 0.00 |
| **43** | 0.01 | 0.00 | 0.01 | 0.00 | 0.01 | 0.01 |
| **44** | 0.03 | 0.00 | 0.03 | 0.03 | 0.00 | 0.00 |
| **45** | 0.01 | 0.00 | 0.01 | 0.01 | 0.00 | 0.01 |
| **46** | 0.01 | 0.00 | 0.01 | 0.01 | 0.01 | 0.00 |
| **47** | 0.01 | 0.00 | 0.01 | 0.01 | 0.01 | 0.01 |
| **48** | 0.07 | 0.07 | 0.00 | 0.00 | 0.00 | 0.00 |
| **49** | 0.03 | 0.03 | 0.00 | 0.00 | 0.00 | 0.03 |
| **50** | 0.03 | 0.03 | 0.00 | 0.00 | 0.03 | 0.00 |
| **51** | 0.01 | 0.01 | 0.00 | 0.00 | 0.01 | 0.01 |
| **52** | 0.03 | 0.03 | 0.00 | 0.03 | 0.00 | 0.00 |
| **53** | 0.01 | 0.01 | 0.00 | 0.01 | 0.00 | 0.01 |
| **54** | 0.01 | 0.01 | 0.00 | 0.01 | 0.01 | 0.00 |
| **55** | 0.01 | 0.01 | 0.00 | 0.01 | 0.01 | 0.01 |
| **56** | 0.03 | 0.03 | 0.03 | 0.00 | 0.00 | 0.00 |
| **57** | 0.01 | 0.01 | 0.01 | 0.00 | 0.00 | 0.01 |
| **58** | 0.01 | 0.01 | 0.01 | 0.00 | 0.01 | 0.00 |
| **59** | 0.01 | 0.01 | 0.01 | 0.00 | 0.01 | 0.01 |
| **60** | 0.01 | 0.01 | 0.01 | 0.01 | 0.00 | 0.00 |
| **61** | 0.01 | 0.01 | 0.01 | 0.01 | 0.00 | 0.01 |
| **62** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.00 |
| **63** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |



Binarize the true labels and store in Y

<pre>
[[0 0 0 0 1 0]
 [1 0 0 0 0 0]
 [1 0 0 0 0 0]
 [0 0 1 0 0 0]
 [0 1 0 0 0 0]
 [0 0 0 0 1 0]]
</pre>


Generate the weak labels with the mixing matrix M

<pre>
[18 48 42 24 21 26]
</pre>


Binarize the weak labels and store in Z

<pre>
[[0 1 0 0 1 0]
 [1 1 0 0 0 0]
 [1 0 1 0 1 0]
 [0 1 1 0 0 0]
 [0 1 0 1 0 1]
 [0 1 1 0 1 0]]
</pre>


## DATA SAMPLE ###

Example of true labels Y

- In decimal y =

<pre>
[4 0 0 2 1 4]
</pre>


- In binary Y =

<pre>
[[0 0 0 0 1 0]
 [1 0 0 0 0 0]
 [1 0 0 0 0 0]
 [0 0 1 0 0 0]
 [0 1 0 0 0 0]
 [0 0 0 0 1 0]]
</pre>


Example of corresponding weak labels Z

- In decimal z =

<pre>
[ 6 62 33  8 50 35]
</pre>


- In binary Z =

<pre>
[[0 0 0 1 1 0]
 [1 1 1 1 1 0]
 [1 0 0 0 0 1]
 [0 0 1 0 0 0]
 [1 1 0 0 1 0]
 [1 0 0 0 1 1]]
</pre>


Prior distribution for **all** true labels. $P(Y)$

<pre>
[ 0.17  0.17  0.17  0.17  0.17  0.17]
</pre>


# EXPECTED ERROR FOR A BASELINE ###

We show the performance of a simple model that always predicts the prior

## BRIER SCORE #
Error matrix with Brier score: $\Psi_{BS} = BS(P(Y), I)$

| | **100000** | **010000** | **001000** | **000100** | **000010** | **000001** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0.17** | 0.69 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 |
| **0.17** | 0.03 | 0.69 | 0.03 | 0.03 | 0.03 | 0.03 |
| **0.17** | 0.03 | 0.03 | 0.70 | 0.03 | 0.03 | 0.03 |
| **0.17** | 0.03 | 0.03 | 0.03 | 0.70 | 0.03 | 0.03 |
| **0.17** | 0.03 | 0.03 | 0.03 | 0.03 | 0.70 | 0.03 |
| **0.17** | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.70 |



Expected Brier score: $\mathbb{E}_{y\sim P(y)} [\Psi_{BS}(S, y)] = \sum_{j=1}^K P(y=j) \Psi_{BS}(S, y_j)$

<pre>
0.8333
</pre>


## LOG-LOSS #
Error matrix with Log-loss: $\Psi_{LL} = LL(P(Y), I)$

| | **100000** | **010000** | **001000** | **000100** | **000010** | **000001** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0.17** | 1.77 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 |
| **0.17** | -0.00 | 1.77 | -0.00 | -0.00 | -0.00 | -0.00 |
| **0.17** | -0.00 | -0.00 | 1.80 | -0.00 | -0.00 | -0.00 |
| **0.17** | -0.00 | -0.00 | -0.00 | 1.80 | -0.00 | -0.00 |
| **0.17** | -0.00 | -0.00 | -0.00 | -0.00 | 1.80 | -0.00 |
| **0.17** | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | 1.80 |



Expected Log-loss: $\mathbb{E}_{y\sim P(y)} [\Psi_{LL}(S, y)] = \sum_{j=1}^K P(y=j) \Psi_{LL}(S, y_j)$

<pre>
1.79165979761
</pre>


# EXAMPLE OF ESTIMATION OF THE MIXING MATRIX M ###

From now, $M_0$ is for the weak set and $M_1$ for the **full set** that contains the true labels

Lets imagine our full set of weak and true labels is this small sample

- z = [ 6 62 33  8 50 35]

- y = [4 0 0 2 1 4]

We can estimate the probability of each weak label given the true
         label by counting first the number of occurrences of both happening at
         the same time

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0 | 0 | 0 | 0 | 0 | 0 |
| **1** | 0 | 0 | 0 | 0 | 0 | 0 |
| **2** | 0 | 0 | 0 | 0 | 0 | 0 |
| **3** | 0 | 0 | 0 | 0 | 0 | 0 |
| **4** | 0 | 0 | 0 | 0 | 0 | 0 |
| **5** | 0 | 0 | 0 | 0 | 0 | 0 |
| **6** | 0 | 0 | 0 | 0 | 1 | 0 |
| **7** | 0 | 0 | 0 | 0 | 0 | 0 |
| **8** | 0 | 0 | 1 | 0 | 0 | 0 |
| **9** | 0 | 0 | 0 | 0 | 0 | 0 |
| **10** | 0 | 0 | 0 | 0 | 0 | 0 |
| **11** | 0 | 0 | 0 | 0 | 0 | 0 |
| **12** | 0 | 0 | 0 | 0 | 0 | 0 |
| **13** | 0 | 0 | 0 | 0 | 0 | 0 |
| **14** | 0 | 0 | 0 | 0 | 0 | 0 |
| **15** | 0 | 0 | 0 | 0 | 0 | 0 |
| **16** | 0 | 0 | 0 | 0 | 0 | 0 |
| **17** | 0 | 0 | 0 | 0 | 0 | 0 |
| **18** | 0 | 0 | 0 | 0 | 0 | 0 |
| **19** | 0 | 0 | 0 | 0 | 0 | 0 |
| **20** | 0 | 0 | 0 | 0 | 0 | 0 |
| **21** | 0 | 0 | 0 | 0 | 0 | 0 |
| **22** | 0 | 0 | 0 | 0 | 0 | 0 |
| **23** | 0 | 0 | 0 | 0 | 0 | 0 |
| **24** | 0 | 0 | 0 | 0 | 0 | 0 |
| **25** | 0 | 0 | 0 | 0 | 0 | 0 |
| **26** | 0 | 0 | 0 | 0 | 0 | 0 |
| **27** | 0 | 0 | 0 | 0 | 0 | 0 |
| **28** | 0 | 0 | 0 | 0 | 0 | 0 |
| **29** | 0 | 0 | 0 | 0 | 0 | 0 |
| **30** | 0 | 0 | 0 | 0 | 0 | 0 |
| **31** | 0 | 0 | 0 | 0 | 0 | 0 |
| **32** | 0 | 0 | 0 | 0 | 0 | 0 |
| **33** | 1 | 0 | 0 | 0 | 0 | 0 |
| **34** | 0 | 0 | 0 | 0 | 0 | 0 |
| **35** | 0 | 0 | 0 | 0 | 1 | 0 |
| **36** | 0 | 0 | 0 | 0 | 0 | 0 |
| **37** | 0 | 0 | 0 | 0 | 0 | 0 |
| **38** | 0 | 0 | 0 | 0 | 0 | 0 |
| **39** | 0 | 0 | 0 | 0 | 0 | 0 |
| **40** | 0 | 0 | 0 | 0 | 0 | 0 |
| **41** | 0 | 0 | 0 | 0 | 0 | 0 |
| **42** | 0 | 0 | 0 | 0 | 0 | 0 |
| **43** | 0 | 0 | 0 | 0 | 0 | 0 |
| **44** | 0 | 0 | 0 | 0 | 0 | 0 |
| **45** | 0 | 0 | 0 | 0 | 0 | 0 |
| **46** | 0 | 0 | 0 | 0 | 0 | 0 |
| **47** | 0 | 0 | 0 | 0 | 0 | 0 |
| **48** | 0 | 0 | 0 | 0 | 0 | 0 |
| **49** | 0 | 0 | 0 | 0 | 0 | 0 |
| **50** | 0 | 1 | 0 | 0 | 0 | 0 |
| **51** | 0 | 0 | 0 | 0 | 0 | 0 |
| **52** | 0 | 0 | 0 | 0 | 0 | 0 |
| **53** | 0 | 0 | 0 | 0 | 0 | 0 |
| **54** | 0 | 0 | 0 | 0 | 0 | 0 |
| **55** | 0 | 0 | 0 | 0 | 0 | 0 |
| **56** | 0 | 0 | 0 | 0 | 0 | 0 |
| **57** | 0 | 0 | 0 | 0 | 0 | 0 |
| **58** | 0 | 0 | 0 | 0 | 0 | 0 |
| **59** | 0 | 0 | 0 | 0 | 0 | 0 |
| **60** | 0 | 0 | 0 | 0 | 0 | 0 |
| **61** | 0 | 0 | 0 | 0 | 0 | 0 |
| **62** | 1 | 0 | 0 | 0 | 0 | 0 |
| **63** | 0 | 0 | 0 | 0 | 0 | 0 |



Where there is one column per true label and one row per each
         possible weak label

Then we can compute the probability of each weak label given the true
         label by dividing every column by its sum. If we do that, we will get
         a possible estimation of $M_0$

Estimated $M_0$

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **1** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **2** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **3** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **4** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **5** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **6** | 0.00 | 0.00 | 0.00 | nan | 0.50 | nan |
| **7** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **8** | 0.00 | 0.00 | 1.00 | nan | 0.00 | nan |
| **9** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **10** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **11** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **12** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **13** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **14** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **15** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **16** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **17** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **18** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **19** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **20** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **21** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **22** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **23** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **24** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **25** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **26** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **27** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **28** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **29** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **30** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **31** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **32** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **33** | 0.50 | 0.00 | 0.00 | nan | 0.00 | nan |
| **34** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **35** | 0.00 | 0.00 | 0.00 | nan | 0.50 | nan |
| **36** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **37** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **38** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **39** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **40** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **41** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **42** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **43** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **44** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **45** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **46** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **47** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **48** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **49** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **50** | 0.00 | 1.00 | 0.00 | nan | 0.00 | nan |
| **51** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **52** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **53** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **54** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **55** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **56** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **57** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **58** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **59** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **60** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **61** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |
| **62** | 0.50 | 0.00 | 0.00 | nan | 0.00 | nan |
| **63** | 0.00 | 0.00 | 0.00 | nan | 0.00 | nan |



However, because given a small data size it is possible that some of
         the weak labels does not occur. We can apply a Laplace correction by
         adding one count to each possible weak label given the true label

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 1 | 1 | 1 | 1 | 1 | 1 |
| **1** | 1 | 1 | 1 | 1 | 1 | 1 |
| **2** | 1 | 1 | 1 | 1 | 1 | 1 |
| **3** | 1 | 1 | 1 | 1 | 1 | 1 |
| **4** | 1 | 1 | 1 | 1 | 1 | 1 |
| **5** | 1 | 1 | 1 | 1 | 1 | 1 |
| **6** | 1 | 1 | 1 | 1 | 2 | 1 |
| **7** | 1 | 1 | 1 | 1 | 1 | 1 |
| **8** | 1 | 1 | 2 | 1 | 1 | 1 |
| **9** | 1 | 1 | 1 | 1 | 1 | 1 |
| **10** | 1 | 1 | 1 | 1 | 1 | 1 |
| **11** | 1 | 1 | 1 | 1 | 1 | 1 |
| **12** | 1 | 1 | 1 | 1 | 1 | 1 |
| **13** | 1 | 1 | 1 | 1 | 1 | 1 |
| **14** | 1 | 1 | 1 | 1 | 1 | 1 |
| **15** | 1 | 1 | 1 | 1 | 1 | 1 |
| **16** | 1 | 1 | 1 | 1 | 1 | 1 |
| **17** | 1 | 1 | 1 | 1 | 1 | 1 |
| **18** | 1 | 1 | 1 | 1 | 1 | 1 |
| **19** | 1 | 1 | 1 | 1 | 1 | 1 |
| **20** | 1 | 1 | 1 | 1 | 1 | 1 |
| **21** | 1 | 1 | 1 | 1 | 1 | 1 |
| **22** | 1 | 1 | 1 | 1 | 1 | 1 |
| **23** | 1 | 1 | 1 | 1 | 1 | 1 |
| **24** | 1 | 1 | 1 | 1 | 1 | 1 |
| **25** | 1 | 1 | 1 | 1 | 1 | 1 |
| **26** | 1 | 1 | 1 | 1 | 1 | 1 |
| **27** | 1 | 1 | 1 | 1 | 1 | 1 |
| **28** | 1 | 1 | 1 | 1 | 1 | 1 |
| **29** | 1 | 1 | 1 | 1 | 1 | 1 |
| **30** | 1 | 1 | 1 | 1 | 1 | 1 |
| **31** | 1 | 1 | 1 | 1 | 1 | 1 |
| **32** | 1 | 1 | 1 | 1 | 1 | 1 |
| **33** | 2 | 1 | 1 | 1 | 1 | 1 |
| **34** | 1 | 1 | 1 | 1 | 1 | 1 |
| **35** | 1 | 1 | 1 | 1 | 2 | 1 |
| **36** | 1 | 1 | 1 | 1 | 1 | 1 |
| **37** | 1 | 1 | 1 | 1 | 1 | 1 |
| **38** | 1 | 1 | 1 | 1 | 1 | 1 |
| **39** | 1 | 1 | 1 | 1 | 1 | 1 |
| **40** | 1 | 1 | 1 | 1 | 1 | 1 |
| **41** | 1 | 1 | 1 | 1 | 1 | 1 |
| **42** | 1 | 1 | 1 | 1 | 1 | 1 |
| **43** | 1 | 1 | 1 | 1 | 1 | 1 |
| **44** | 1 | 1 | 1 | 1 | 1 | 1 |
| **45** | 1 | 1 | 1 | 1 | 1 | 1 |
| **46** | 1 | 1 | 1 | 1 | 1 | 1 |
| **47** | 1 | 1 | 1 | 1 | 1 | 1 |
| **48** | 1 | 1 | 1 | 1 | 1 | 1 |
| **49** | 1 | 1 | 1 | 1 | 1 | 1 |
| **50** | 1 | 2 | 1 | 1 | 1 | 1 |
| **51** | 1 | 1 | 1 | 1 | 1 | 1 |
| **52** | 1 | 1 | 1 | 1 | 1 | 1 |
| **53** | 1 | 1 | 1 | 1 | 1 | 1 |
| **54** | 1 | 1 | 1 | 1 | 1 | 1 |
| **55** | 1 | 1 | 1 | 1 | 1 | 1 |
| **56** | 1 | 1 | 1 | 1 | 1 | 1 |
| **57** | 1 | 1 | 1 | 1 | 1 | 1 |
| **58** | 1 | 1 | 1 | 1 | 1 | 1 |
| **59** | 1 | 1 | 1 | 1 | 1 | 1 |
| **60** | 1 | 1 | 1 | 1 | 1 | 1 |
| **61** | 1 | 1 | 1 | 1 | 1 | 1 |
| **62** | 2 | 1 | 1 | 1 | 1 | 1 |
| **63** | 1 | 1 | 1 | 1 | 1 | 1 |



Estimated $M_0$ with Laplace correction

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **1** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **2** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **3** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **4** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **5** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **6** | 0.02 | 0.02 | 0.02 | 0.02 | 0.03 | 0.02 |
| **7** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **8** | 0.02 | 0.02 | 0.03 | 0.02 | 0.02 | 0.02 |
| **9** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **10** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **11** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **12** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **13** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **14** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **15** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **16** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **17** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **18** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **19** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **20** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **21** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **22** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **23** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **24** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **25** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **26** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **27** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **28** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **29** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **30** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **31** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **32** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **33** | 0.03 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **34** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **35** | 0.02 | 0.02 | 0.02 | 0.02 | 0.03 | 0.02 |
| **36** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **37** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **38** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **39** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **40** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **41** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **42** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **43** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **44** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **45** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **46** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **47** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **48** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **49** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **50** | 0.02 | 0.03 | 0.02 | 0.02 | 0.02 | 0.02 |
| **51** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **52** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **53** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **54** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **55** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **56** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **57** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **58** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **59** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **60** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **61** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **62** | 0.03 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |
| **63** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 |



The mixing matrix for the clean data $M_1$

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **1** | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **2** | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 |
| **3** | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 |
| **4** | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 |
| **5** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 |



# ESTIMATION OF THE MIXING MATRIX M FOR ALL DATA ###

Now lets do the same but with the full set of weak and true labels

This is the count

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0 | 0 | 0 | 0 | 0 | 0 |
| **1** | 0 | 0 | 0 | 0 | 0 | 4 |
| **2** | 0 | 0 | 0 | 0 | 8 | 0 |
| **3** | 0 | 0 | 0 | 0 | 4 | 3 |
| **4** | 0 | 0 | 0 | 3 | 0 | 0 |
| **5** | 0 | 0 | 0 | 3 | 0 | 5 |
| **6** | 0 | 0 | 0 | 4 | 4 | 0 |
| **7** | 0 | 0 | 0 | 1 | 0 | 1 |
| **8** | 0 | 0 | 7 | 0 | 0 | 0 |
| **9** | 0 | 0 | 2 | 0 | 0 | 3 |
| **10** | 0 | 0 | 4 | 0 | 1 | 0 |
| **11** | 0 | 0 | 2 | 0 | 1 | 1 |
| **12** | 0 | 0 | 1 | 3 | 0 | 0 |
| **13** | 0 | 0 | 0 | 1 | 0 | 0 |
| **14** | 0 | 0 | 1 | 2 | 0 | 0 |
| **15** | 0 | 0 | 0 | 0 | 1 | 0 |
| **16** | 0 | 9 | 0 | 0 | 0 | 0 |
| **17** | 0 | 2 | 0 | 0 | 0 | 3 |
| **18** | 0 | 8 | 0 | 0 | 0 | 0 |
| **19** | 0 | 2 | 0 | 0 | 0 | 1 |
| **20** | 0 | 0 | 0 | 1 | 0 | 0 |
| **21** | 0 | 1 | 0 | 0 | 0 | 0 |
| **22** | 0 | 0 | 0 | 0 | 2 | 0 |
| **23** | 0 | 0 | 0 | 0 | 0 | 0 |
| **24** | 0 | 3 | 2 | 0 | 0 | 0 |
| **25** | 0 | 0 | 0 | 0 | 0 | 0 |
| **26** | 0 | 2 | 0 | 0 | 2 | 0 |
| **27** | 0 | 0 | 0 | 0 | 1 | 0 |
| **28** | 0 | 1 | 1 | 0 | 0 | 0 |
| **29** | 0 | 0 | 1 | 1 | 0 | 0 |
| **30** | 0 | 0 | 0 | 2 | 0 | 0 |
| **31** | 0 | 1 | 0 | 0 | 0 | 0 |
| **32** | 4 | 0 | 0 | 0 | 0 | 0 |
| **33** | 3 | 0 | 0 | 0 | 0 | 2 |
| **34** | 3 | 0 | 0 | 0 | 2 | 0 |
| **35** | 0 | 0 | 0 | 0 | 2 | 0 |
| **36** | 4 | 0 | 0 | 1 | 0 | 0 |
| **37** | 1 | 0 | 0 | 1 | 0 | 0 |
| **38** | 1 | 0 | 0 | 1 | 1 | 0 |
| **39** | 1 | 0 | 0 | 0 | 1 | 0 |
| **40** | 3 | 0 | 0 | 0 | 0 | 0 |
| **41** | 1 | 0 | 4 | 0 | 0 | 2 |
| **42** | 2 | 0 | 2 | 0 | 2 | 0 |
| **43** | 1 | 0 | 0 | 0 | 0 | 0 |
| **44** | 1 | 0 | 1 | 0 | 0 | 0 |
| **45** | 1 | 0 | 2 | 1 | 0 | 1 |
| **46** | 0 | 0 | 0 | 1 | 0 | 0 |
| **47** | 1 | 0 | 0 | 1 | 0 | 0 |
| **48** | 1 | 1 | 0 | 0 | 0 | 0 |
| **49** | 0 | 0 | 0 | 0 | 0 | 1 |
| **50** | 1 | 2 | 0 | 0 | 0 | 0 |
| **51** | 0 | 0 | 0 | 0 | 0 | 2 |
| **52** | 0 | 0 | 0 | 2 | 0 | 0 |
| **53** | 0 | 0 | 0 | 0 | 0 | 1 |
| **54** | 0 | 0 | 0 | 2 | 0 | 0 |
| **55** | 0 | 1 | 0 | 1 | 0 | 0 |
| **56** | 2 | 0 | 1 | 0 | 0 | 0 |
| **57** | 0 | 1 | 0 | 0 | 0 | 3 |
| **58** | 1 | 0 | 1 | 0 | 0 | 0 |
| **59** | 0 | 0 | 0 | 0 | 1 | 0 |
| **60** | 1 | 0 | 1 | 1 | 0 | 0 |
| **61** | 0 | 0 | 0 | 0 | 0 | 0 |
| **62** | 1 | 0 | 0 | 0 | 0 | 0 |
| **63** | 0 | 0 | 0 | 0 | 0 | 0 |



Estimated $M_0$ without Laplace correction

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **1** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.12 |
| **2** | 0.00 | 0.00 | 0.00 | 0.00 | 0.24 | 0.00 |
| **3** | 0.00 | 0.00 | 0.00 | 0.00 | 0.12 | 0.09 |
| **4** | 0.00 | 0.00 | 0.00 | 0.09 | 0.00 | 0.00 |
| **5** | 0.00 | 0.00 | 0.00 | 0.09 | 0.00 | 0.15 |
| **6** | 0.00 | 0.00 | 0.00 | 0.12 | 0.12 | 0.00 |
| **7** | 0.00 | 0.00 | 0.00 | 0.03 | 0.00 | 0.03 |
| **8** | 0.00 | 0.00 | 0.21 | 0.00 | 0.00 | 0.00 |
| **9** | 0.00 | 0.00 | 0.06 | 0.00 | 0.00 | 0.09 |
| **10** | 0.00 | 0.00 | 0.12 | 0.00 | 0.03 | 0.00 |
| **11** | 0.00 | 0.00 | 0.06 | 0.00 | 0.03 | 0.03 |
| **12** | 0.00 | 0.00 | 0.03 | 0.09 | 0.00 | 0.00 |
| **13** | 0.00 | 0.00 | 0.00 | 0.03 | 0.00 | 0.00 |
| **14** | 0.00 | 0.00 | 0.03 | 0.06 | 0.00 | 0.00 |
| **15** | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 | 0.00 |
| **16** | 0.00 | 0.26 | 0.00 | 0.00 | 0.00 | 0.00 |
| **17** | 0.00 | 0.06 | 0.00 | 0.00 | 0.00 | 0.09 |
| **18** | 0.00 | 0.24 | 0.00 | 0.00 | 0.00 | 0.00 |
| **19** | 0.00 | 0.06 | 0.00 | 0.00 | 0.00 | 0.03 |
| **20** | 0.00 | 0.00 | 0.00 | 0.03 | 0.00 | 0.00 |
| **21** | 0.00 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 |
| **22** | 0.00 | 0.00 | 0.00 | 0.00 | 0.06 | 0.00 |
| **23** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **24** | 0.00 | 0.09 | 0.06 | 0.00 | 0.00 | 0.00 |
| **25** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **26** | 0.00 | 0.06 | 0.00 | 0.00 | 0.06 | 0.00 |
| **27** | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 | 0.00 |
| **28** | 0.00 | 0.03 | 0.03 | 0.00 | 0.00 | 0.00 |
| **29** | 0.00 | 0.00 | 0.03 | 0.03 | 0.00 | 0.00 |
| **30** | 0.00 | 0.00 | 0.00 | 0.06 | 0.00 | 0.00 |
| **31** | 0.00 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 |
| **32** | 0.12 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **33** | 0.09 | 0.00 | 0.00 | 0.00 | 0.00 | 0.06 |
| **34** | 0.09 | 0.00 | 0.00 | 0.00 | 0.06 | 0.00 |
| **35** | 0.00 | 0.00 | 0.00 | 0.00 | 0.06 | 0.00 |
| **36** | 0.12 | 0.00 | 0.00 | 0.03 | 0.00 | 0.00 |
| **37** | 0.03 | 0.00 | 0.00 | 0.03 | 0.00 | 0.00 |
| **38** | 0.03 | 0.00 | 0.00 | 0.03 | 0.03 | 0.00 |
| **39** | 0.03 | 0.00 | 0.00 | 0.00 | 0.03 | 0.00 |
| **40** | 0.09 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **41** | 0.03 | 0.00 | 0.12 | 0.00 | 0.00 | 0.06 |
| **42** | 0.06 | 0.00 | 0.06 | 0.00 | 0.06 | 0.00 |
| **43** | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **44** | 0.03 | 0.00 | 0.03 | 0.00 | 0.00 | 0.00 |
| **45** | 0.03 | 0.00 | 0.06 | 0.03 | 0.00 | 0.03 |
| **46** | 0.00 | 0.00 | 0.00 | 0.03 | 0.00 | 0.00 |
| **47** | 0.03 | 0.00 | 0.00 | 0.03 | 0.00 | 0.00 |
| **48** | 0.03 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 |
| **49** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 |
| **50** | 0.03 | 0.06 | 0.00 | 0.00 | 0.00 | 0.00 |
| **51** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.06 |
| **52** | 0.00 | 0.00 | 0.00 | 0.06 | 0.00 | 0.00 |
| **53** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 |
| **54** | 0.00 | 0.00 | 0.00 | 0.06 | 0.00 | 0.00 |
| **55** | 0.00 | 0.03 | 0.00 | 0.03 | 0.00 | 0.00 |
| **56** | 0.06 | 0.00 | 0.03 | 0.00 | 0.00 | 0.00 |
| **57** | 0.00 | 0.03 | 0.00 | 0.00 | 0.00 | 0.09 |
| **58** | 0.03 | 0.00 | 0.03 | 0.00 | 0.00 | 0.00 |
| **59** | 0.00 | 0.00 | 0.00 | 0.00 | 0.03 | 0.00 |
| **60** | 0.03 | 0.00 | 0.03 | 0.03 | 0.00 | 0.00 |
| **61** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **62** | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **63** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |



Estimated $M_0$ with Laplace correction

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **1** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.05 |
| **2** | 0.01 | 0.01 | 0.01 | 0.01 | 0.09 | 0.01 |
| **3** | 0.01 | 0.01 | 0.01 | 0.01 | 0.05 | 0.04 |
| **4** | 0.01 | 0.01 | 0.01 | 0.04 | 0.01 | 0.01 |
| **5** | 0.01 | 0.01 | 0.01 | 0.04 | 0.01 | 0.06 |
| **6** | 0.01 | 0.01 | 0.01 | 0.05 | 0.05 | 0.01 |
| **7** | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 | 0.02 |
| **8** | 0.01 | 0.01 | 0.08 | 0.01 | 0.01 | 0.01 |
| **9** | 0.01 | 0.01 | 0.03 | 0.01 | 0.01 | 0.04 |
| **10** | 0.01 | 0.01 | 0.05 | 0.01 | 0.02 | 0.01 |
| **11** | 0.01 | 0.01 | 0.03 | 0.01 | 0.02 | 0.02 |
| **12** | 0.01 | 0.01 | 0.02 | 0.04 | 0.01 | 0.01 |
| **13** | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **14** | 0.01 | 0.01 | 0.02 | 0.03 | 0.01 | 0.01 |
| **15** | 0.01 | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 |
| **16** | 0.01 | 0.10 | 0.01 | 0.01 | 0.01 | 0.01 |
| **17** | 0.01 | 0.03 | 0.01 | 0.01 | 0.01 | 0.04 |
| **18** | 0.01 | 0.09 | 0.01 | 0.01 | 0.01 | 0.01 |
| **19** | 0.01 | 0.03 | 0.01 | 0.01 | 0.01 | 0.02 |
| **20** | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **21** | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 |
| **22** | 0.01 | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 |
| **23** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **24** | 0.01 | 0.04 | 0.03 | 0.01 | 0.01 | 0.01 |
| **25** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **26** | 0.01 | 0.03 | 0.01 | 0.01 | 0.03 | 0.01 |
| **27** | 0.01 | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 |
| **28** | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 | 0.01 |
| **29** | 0.01 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 |
| **30** | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 | 0.01 |
| **31** | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 |
| **32** | 0.05 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **33** | 0.04 | 0.01 | 0.01 | 0.01 | 0.01 | 0.03 |
| **34** | 0.04 | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 |
| **35** | 0.01 | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 |
| **36** | 0.05 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **37** | 0.02 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **38** | 0.02 | 0.01 | 0.01 | 0.02 | 0.02 | 0.01 |
| **39** | 0.02 | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 |
| **40** | 0.04 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **41** | 0.02 | 0.01 | 0.05 | 0.01 | 0.01 | 0.03 |
| **42** | 0.03 | 0.01 | 0.03 | 0.01 | 0.03 | 0.01 |
| **43** | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **44** | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 |
| **45** | 0.02 | 0.01 | 0.03 | 0.02 | 0.01 | 0.02 |
| **46** | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **47** | 0.02 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **48** | 0.02 | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 |
| **49** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.02 |
| **50** | 0.02 | 0.03 | 0.01 | 0.01 | 0.01 | 0.01 |
| **51** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.03 |
| **52** | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 | 0.01 |
| **53** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.02 |
| **54** | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 | 0.01 |
| **55** | 0.01 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 |
| **56** | 0.03 | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 |
| **57** | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 | 0.04 |
| **58** | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 |
| **59** | 0.01 | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 |
| **60** | 0.02 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 |
| **61** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **62** | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **63** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |



The mixing matrix for the clean data $M_1$

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **1** | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **2** | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 |
| **3** | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 |
| **4** | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 |
| **5** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 |



## COMBINATION OF BOTH MATRICES FOR THE FULL DATASET ###

Proportion of samples with weak ($q_0$) and with true labels ($q_1$)

- $q_0$ = 0.9
- $q_1$ = 0.1

Composition of mixing matrices $M = [q_0*M_0 \mathtt{ , } q_1*M_1]^T$

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **1** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.05 |
| **2** | 0.01 | 0.01 | 0.01 | 0.01 | 0.08 | 0.01 |
| **3** | 0.01 | 0.01 | 0.01 | 0.01 | 0.05 | 0.04 |
| **4** | 0.01 | 0.01 | 0.01 | 0.04 | 0.01 | 0.01 |
| **5** | 0.01 | 0.01 | 0.01 | 0.04 | 0.01 | 0.06 |
| **6** | 0.01 | 0.01 | 0.01 | 0.05 | 0.05 | 0.01 |
| **7** | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 | 0.02 |
| **8** | 0.01 | 0.01 | 0.07 | 0.01 | 0.01 | 0.01 |
| **9** | 0.01 | 0.01 | 0.03 | 0.01 | 0.01 | 0.04 |
| **10** | 0.01 | 0.01 | 0.05 | 0.01 | 0.02 | 0.01 |
| **11** | 0.01 | 0.01 | 0.03 | 0.01 | 0.02 | 0.02 |
| **12** | 0.01 | 0.01 | 0.02 | 0.04 | 0.01 | 0.01 |
| **13** | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **14** | 0.01 | 0.01 | 0.02 | 0.03 | 0.01 | 0.01 |
| **15** | 0.01 | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 |
| **16** | 0.01 | 0.09 | 0.01 | 0.01 | 0.01 | 0.01 |
| **17** | 0.01 | 0.03 | 0.01 | 0.01 | 0.01 | 0.04 |
| **18** | 0.01 | 0.08 | 0.01 | 0.01 | 0.01 | 0.01 |
| **19** | 0.01 | 0.03 | 0.01 | 0.01 | 0.01 | 0.02 |
| **20** | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **21** | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 |
| **22** | 0.01 | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 |
| **23** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **24** | 0.01 | 0.04 | 0.03 | 0.01 | 0.01 | 0.01 |
| **25** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **26** | 0.01 | 0.03 | 0.01 | 0.01 | 0.03 | 0.01 |
| **27** | 0.01 | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 |
| **28** | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 | 0.01 |
| **29** | 0.01 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 |
| **30** | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 | 0.01 |
| **31** | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 |
| **32** | 0.05 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **33** | 0.04 | 0.01 | 0.01 | 0.01 | 0.01 | 0.03 |
| **34** | 0.04 | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 |
| **35** | 0.01 | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 |
| **36** | 0.05 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **37** | 0.02 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **38** | 0.02 | 0.01 | 0.01 | 0.02 | 0.02 | 0.01 |
| **39** | 0.02 | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 |
| **40** | 0.04 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **41** | 0.02 | 0.01 | 0.05 | 0.01 | 0.01 | 0.03 |
| **42** | 0.03 | 0.01 | 0.03 | 0.01 | 0.03 | 0.01 |
| **43** | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **44** | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 |
| **45** | 0.02 | 0.01 | 0.03 | 0.02 | 0.01 | 0.02 |
| **46** | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **47** | 0.02 | 0.01 | 0.01 | 0.02 | 0.01 | 0.01 |
| **48** | 0.02 | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 |
| **49** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.02 |
| **50** | 0.02 | 0.03 | 0.01 | 0.01 | 0.01 | 0.01 |
| **51** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.03 |
| **52** | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 | 0.01 |
| **53** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.02 |
| **54** | 0.01 | 0.01 | 0.01 | 0.03 | 0.01 | 0.01 |
| **55** | 0.01 | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 |
| **56** | 0.03 | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 |
| **57** | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 | 0.04 |
| **58** | 0.02 | 0.01 | 0.02 | 0.01 | 0.01 | 0.01 |
| **59** | 0.01 | 0.01 | 0.01 | 0.01 | 0.02 | 0.01 |
| **60** | 0.02 | 0.01 | 0.02 | 0.02 | 0.01 | 0.01 |
| **61** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **62** | 0.02 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **63** | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 | 0.01 |
| **64** | 0.10 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **65** | 0.00 | 0.10 | 0.00 | 0.00 | 0.00 | 0.00 |
| **66** | 0.00 | 0.00 | 0.10 | 0.00 | 0.00 | 0.00 |
| **67** | 0.00 | 0.00 | 0.00 | 0.10 | 0.00 | 0.00 |
| **68** | 0.00 | 0.00 | 0.00 | 0.00 | 0.10 | 0.00 |
| **69** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.10 |



The corresponding indices of the weak labels to the rows of the matrix M

<pre>
[ 6 62 33  8 50 35]
</pre>


The corresponding indices of the true labels to the rows of the matrix M

<pre>
[68 64 64 66 65 68]
</pre>


# TRAINING SOME MODELS ###

From here I will add an extra feature fixed to 1 for the bias

<pre>
[[ 9.28 -3.83  1.  ]
 [ 0.34  3.91  1.  ]
 [ 1.26  4.21  1.  ]
 [-0.75  2.63  1.  ]
 [ 1.16 -1.07  1.  ]
 [ 8.54 -2.97  1.  ]]
</pre>


## SIMULATION OF GRADIENT DESCENT FOR A LR ###

Initial weights w

<pre>
[[ 1.76  0.4   0.98  2.24  1.87 -0.98]
 [ 0.95 -0.15 -0.1   0.41  0.14  1.45]
 [ 0.76  0.12  0.44  0.33  1.49 -0.21]]
</pre>


The small sample of X

<pre>
[[ 9.28 -3.83  1.  ]
 [ 0.34  3.91  1.  ]
 [ 1.26  4.21  1.  ]
 [-0.75  2.63  1.  ]
 [ 1.16 -1.07  1.  ]
 [ 8.54 -2.97  1.  ]]
</pre>


Initial model activations m_a = X m_W

<pre>
[[  1.35e+01   4.41e+00   9.92e+00   1.95e+01   1.83e+01  -1.48e+01]
 [  5.07e+00  -3.34e-01   3.72e-01   2.70e+00   2.69e+00   5.14e+00]
 [  6.97e+00  -1.22e-02   1.24e+00   4.88e+00   4.45e+00   4.68e+00]
 [  1.93e+00  -5.76e-01  -5.61e-01  -2.68e-01   4.72e-01   4.35e+00]
 [  1.79e+00   7.47e-01   1.69e+00   2.49e+00   3.50e+00  -2.89e+00]
 [  1.30e+01   3.99e+00   9.11e+00   1.82e+01   1.70e+01  -1.29e+01]]
</pre>


Initial model outputs m_q = softmax(m_a)

<pre>
[[  1.82e-03   2.09e-07   5.15e-05   7.81e-01   2.17e-01   9.06e-16]
 [  4.39e-01   1.98e-03   4.00e-03   4.09e-02   4.06e-02   4.73e-01]
 [  7.64e-01   7.08e-04   2.47e-03   9.39e-02   6.11e-02   7.75e-02]
 [  7.89e-02   6.41e-03   6.51e-03   8.72e-03   1.83e-02   8.81e-01]
 [  1.02e-01   3.58e-02   9.17e-02   2.05e-01   5.64e-01   9.46e-04]
 [  4.08e-03   4.94e-07   8.26e-05   7.72e-01   2.24e-01   2.38e-14]]
</pre>


Initial model predictions m_pred = argmax(m_q)

<pre>
[3 5 0 5 4 3]
</pre>


Compute the error of the predicted probabilities q true labels Y

<pre>
[ 1.22  0.54  0.07  1.77  1.31  1.2 ]
</pre>


Compute the gradient of the Brier score

<pre>
[[ -0.38  -1.12   0.86  14.2  -13.16  -0.4 ]
 [ -3.1    1.05  -2.68  -4.92   5.16   4.49]
 [ -0.61  -0.96  -0.9    1.9   -0.87   1.43]]
</pre>


Update the weights $W_{t+1} = W_t - G$

<pre>
[[  2.14   1.52   0.12 -11.96  15.02  -0.58]
 [  4.05  -1.21   2.58   5.33  -5.02  -3.03]
 [  1.37   1.08   1.34  -1.57   2.37  -1.64]]
</pre>


### WE CAN PERFORM GD FOR SOME ITERATIONS FULL TRUE SET ###

Iterations :

| it | mean bs | acc |
| :--- | :-----------: | :-----: |
| 0 | 1.11E+00 | 0.42 |
| 1 | 1.42E+00 | 0.29 |
| 2 | 1.34E+00 | 0.33 |
| 3 | 1.33E+00 | 0.34 |
| 4 | 1.49E+00 | 0.26 |
| 5 | 1.20E+00 | 0.40 |
| 6 | 1.09E+00 | 0.46 |
| 7 | 1.32E+00 | 0.34 |
| 8 | 1.17E+00 | 0.41 |
| 9 | 1.10E+00 | 0.45 |
| 10 | 1.20E+00 | 0.40 |
| 11 | 1.27E+00 | 0.36 |
| 12 | 1.06E+00 | 0.47 |
| 13 | 1.02E+00 | 0.49 |
| 14 | 9.89E-01 | 0.51 |
| 15 | 8.40E-01 | 0.58 |
| 16 | 9.61E-01 | 0.52 |
| 17 | 1.14E+00 | 0.43 |
| 18 | 9.69E-01 | 0.52 |
| 19 | 1.03E+00 | 0.48 |

## SIMULATION OF THE EXPECTATION MAXIMIZATION STEPS FOR A LR ###

Initial weights w

<pre>
[[ 1.76  0.4   0.98  2.24  1.87 -0.98]
 [ 0.95 -0.15 -0.1   0.41  0.14  1.45]
 [ 0.76  0.12  0.44  0.33  1.49 -0.21]]
</pre>


The small sample of X

<pre>
[[ 9.28 -3.83  1.  ]
 [ 0.34  3.91  1.  ]
 [ 1.26  4.21  1.  ]
 [-0.75  2.63  1.  ]
 [ 1.16 -1.07  1.  ]
 [ 8.54 -2.97  1.  ]]
</pre>


#### EXPECTATION STEP: Assume the weights of the model are right ###

Initial prediction Z = X w

<pre>
[[  1.35e+01   4.41e+00   9.92e+00   1.95e+01   1.83e+01  -1.48e+01]
 [  5.07e+00  -3.34e-01   3.72e-01   2.70e+00   2.69e+00   5.14e+00]
 [  6.97e+00  -1.22e-02   1.24e+00   4.88e+00   4.45e+00   4.68e+00]
 [  1.93e+00  -5.76e-01  -5.61e-01  -2.68e-01   4.72e-01   4.35e+00]
 [  1.79e+00   7.47e-01   1.69e+00   2.49e+00   3.50e+00  -2.89e+00]
 [  1.30e+01   3.99e+00   9.11e+00   1.82e+01   1.70e+01  -1.29e+01]]
</pre>


Initial q = softmax(Z)

<pre>
[[  1.82e-03   2.09e-07   5.15e-05   7.81e-01   2.17e-01   9.06e-16]
 [  4.39e-01   1.98e-03   4.00e-03   4.09e-02   4.06e-02   4.73e-01]
 [  7.64e-01   7.08e-04   2.47e-03   9.39e-02   6.11e-02   7.75e-02]
 [  7.89e-02   6.41e-03   6.51e-03   8.72e-03   1.83e-02   8.81e-01]
 [  1.02e-01   3.58e-02   9.17e-02   2.05e-01   5.64e-01   9.46e-04]
 [  4.08e-03   4.94e-07   8.26e-05   7.72e-01   2.24e-01   2.38e-14]]
</pre>


Initial predictions

<pre>
[3 5 0 5 4 3]
</pre>


Compute the virtual labels Q = q * M[Z_index]

<pre>
[[  1.67e-05   1.92e-09   4.78e-07   3.62e-02   1.01e-02   8.40e-18]
 [  8.07e-03   1.81e-05   3.71e-05   3.80e-04   3.77e-04   4.39e-03]
 [  2.81e-02   6.50e-06   2.29e-05   8.71e-04   5.67e-04   2.16e-03]
 [  7.25e-04   5.89e-05   4.83e-04   8.09e-05   1.70e-04   8.18e-03]
 [  1.87e-03   9.86e-04   8.51e-04   1.90e-03   5.24e-03   8.77e-06]
 [  3.74e-05   4.54e-09   7.66e-07   7.16e-03   6.24e-03   2.21e-16]]
</pre>


Normalize the virtual labels to sum to one

<pre>
[[  3.60e-04   4.15e-08   1.03e-05   7.82e-01   2.17e-01   1.81e-16]
 [  6.08e-01   1.37e-03   2.80e-03   2.86e-02   2.84e-02   3.31e-01]
 [  8.86e-01   2.05e-04   7.24e-04   2.75e-02   1.79e-02   6.80e-02]
 [  7.47e-02   6.07e-03   4.98e-02   8.35e-03   1.75e-02   8.44e-01]
 [  1.72e-01   9.08e-02   7.84e-02   1.75e-01   4.82e-01   8.08e-04]
 [  2.79e-03   3.38e-07   5.70e-05   5.33e-01   4.64e-01   1.64e-14]]
</pre>


Search for infinite or NaN values and remove them for this training step
The following samples are NOT removed from the training

<pre>
[0 1 2 3 4 5]
</pre>


#### MAXIMIZATION STEP: In our case update the weights to MINIMIZE the error###

Compute the error of the predicted probabilities q against the new Virtual labels Q

<pre>
[  3.60e-06   4.90e-02   2.11e-02   3.31e-03   1.58e-02   1.15e-01]
</pre>


Compute the gradient of the Brier score

<pre>
[[-0.08 -0.   -0.    0.73 -0.67  0.03]
 [-0.39  0.    0.   -0.18  0.26  0.31]
 [-0.12 -0.   -0.    0.09 -0.05  0.08]]
</pre>


Update the weights $W_{t+1} = W_t - G$

<pre>
[[ 1.84  0.4   0.98  1.51  2.54 -1.01]
 [ 1.34 -0.15 -0.11  0.59 -0.11  1.14]
 [ 0.88  0.13  0.45  0.24  1.55 -0.29]]
</pre>


### WE CAN PERFORM EM FOR SOME ITERATIONS ###

Iterations :

| it | mean bs | acc |
| :-- | :-------: | :---: |
| 0 | 1.22E-02 | 0.32
| 1 | 3.97E-03 | 0.34
| 2 | 2.74E-03 | 0.34
| 3 | 1.86E-03 | 0.34
| 4 | 1.98E-03 | 0.34
| 5 | 2.21E-03 | 0.34
| 6 | 2.48E-03 | 0.34
| 7 | 2.90E-03 | 0.34
| 8 | 4.83E-03 | 0.34
| 9 | 6.54E-03 | 0.17
| 10 | 4.63E-03 | 0.34
| 11 | 1.03E-02 | 0.34
| 12 | 2.46E-02 | 0.17
| 13 | 3.76E-02 | 0.46
| 14 | 3.37E-02 | 0.37
| 15 | 1.65E-03 | 0.32
| 16 | 8.97E-04 | 0.32
| 17 | 9.51E-04 | 0.32
| 18 | 9.83E-04 | 0.32
| 19 | 9.97E-04 | 0.32
