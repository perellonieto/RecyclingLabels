Most of the following floats are limited to a precision of 2 decimals

# DATA DESCRIPTION ###

- name = webs
- Number of features = 2099
- Number of classes = 6
- Classes = ['blog', 'inmo', 'parking', 'b2c', 'no_b2c', 'Other']
- Samples with only weak = 65912
- Samples with true labels = 1249

## DATA SAMPLE ###

Example of true labels Y

- In decimal y =

<pre>
[4 5 5 4 0 5]
</pre>


- In binary Y =

<pre>
[[0 0 0 0 1 0]
 [0 0 0 0 0 1]
 [0 0 0 0 0 1]
 [0 0 0 0 1 0]
 [1 0 0 0 0 0]
 [0 0 0 0 0 1]]
</pre>


Example of corresponding weak labels Z

- In decimal z =

<pre>
[1 6 6 2 6 1]
</pre>


- In binary Z =

<pre>
[[0 0 0 0 0 1]
 [0 0 0 1 1 0]
 [0 0 0 1 1 0]
 [0 0 0 0 1 0]
 [0 0 0 1 1 0]
 [0 0 0 0 0 1]]
</pre>


Prior distribution for **all** true labels. $P(Y)$

<pre>
[ 0.02  0.03  0.12  0.13  0.56  0.14]
</pre>


# EXPECTED ERROR FOR A BASELINE ###

We show the performance of a simple model that always predicts the prior

## BRIER SCORE #
Error matrix with Brier score: $\Psi_{BS} = BS(P(Y), I)$

| | **100000** | **010000** | **001000** | **000100** | **000010** | **000001** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0.02** | 0.96 | 0.00 | 0.02 | 0.02 | 0.31 | 0.02 |
| **0.03** | 0.00 | 0.94 | 0.02 | 0.02 | 0.31 | 0.02 |
| **0.12** | 0.00 | 0.00 | 0.77 | 0.02 | 0.31 | 0.02 |
| **0.13** | 0.00 | 0.00 | 0.02 | 0.76 | 0.31 | 0.02 |
| **0.56** | 0.00 | 0.00 | 0.02 | 0.02 | 0.19 | 0.02 |
| **0.14** | 0.00 | 0.00 | 0.02 | 0.02 | 0.31 | 0.74 |



Expected Brier score: $\mathbb{E}_{y\sim P(y)} [\Psi_{BS}(S, y)] = \sum_{j=1}^K P(y=j) \Psi_{BS}(S, y_j)$

<pre>
0.633740619397
</pre>


## LOG-LOSS #
Error matrix with Log-loss: $\Psi_{LL} = LL(P(Y), I)$

| | **100000** | **010000** | **001000** | **000100** | **000010** | **000001** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0.02** | 3.99 | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 |
| **0.03** | -0.00 | 3.47 | -0.00 | -0.00 | -0.00 | -0.00 |
| **0.12** | -0.00 | -0.00 | 2.10 | -0.00 | -0.00 | -0.00 |
| **0.13** | -0.00 | -0.00 | -0.00 | 2.06 | -0.00 | -0.00 |
| **0.56** | -0.00 | -0.00 | -0.00 | -0.00 | 0.58 | -0.00 |
| **0.14** | -0.00 | -0.00 | -0.00 | -0.00 | -0.00 | 1.97 |



Expected Log-loss: $\mathbb{E}_{y\sim P(y)} [\Psi_{LL}(S, y)] = \sum_{j=1}^K P(y=j) \Psi_{LL}(S, y_j)$

<pre>
1.30127399351
</pre>


# EXAMPLE OF ESTIMATION OF THE MIXING MATRIX M ###

From now, $M_0$ is for the weak set and $M_1$ for the **full set** that contains the true labels

Lets imagine our full set of weak and true labels is this small sample

- z = [1 6 6 2 6 1]

- y = [4 5 5 4 0 5]

We can estimate the probability of each weak label given the true
         label by counting first the number of occurrences of both happening at
         the same time

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0 | 0 | 0 | 0 | 0 | 0 |
| **1** | 0 | 0 | 0 | 0 | 1 | 1 |
| **2** | 0 | 0 | 0 | 0 | 1 | 0 |
| **3** | 0 | 0 | 0 | 0 | 0 | 0 |
| **4** | 0 | 0 | 0 | 0 | 0 | 0 |
| **5** | 0 | 0 | 0 | 0 | 0 | 0 |
| **6** | 1 | 0 | 0 | 0 | 0 | 2 |
| **7** | 0 | 0 | 0 | 0 | 0 | 0 |
| **8** | 0 | 0 | 0 | 0 | 0 | 0 |
| **9** | 0 | 0 | 0 | 0 | 0 | 0 |
| **10** | 0 | 0 | 0 | 0 | 0 | 0 |
| **11** | 0 | 0 | 0 | 0 | 0 | 0 |
| **12** | 0 | 0 | 0 | 0 | 0 | 0 |
| **13** | 0 | 0 | 0 | 0 | 0 | 0 |
| **14** | 0 | 0 | 0 | 0 | 0 | 0 |
| **15** | 0 | 0 | 0 | 0 | 0 | 0 |
| **16** | 0 | 0 | 0 | 0 | 0 | 0 |
| **17** | 0 | 0 | 0 | 0 | 0 | 0 |
| **18** | 0 | 0 | 0 | 0 | 0 | 0 |
| **19** | 0 | 0 | 0 | 0 | 0 | 0 |
| **20** | 0 | 0 | 0 | 0 | 0 | 0 |
| **21** | 0 | 0 | 0 | 0 | 0 | 0 |
| **22** | 0 | 0 | 0 | 0 | 0 | 0 |
| **23** | 0 | 0 | 0 | 0 | 0 | 0 |
| **24** | 0 | 0 | 0 | 0 | 0 | 0 |
| **25** | 0 | 0 | 0 | 0 | 0 | 0 |
| **26** | 0 | 0 | 0 | 0 | 0 | 0 |
| **27** | 0 | 0 | 0 | 0 | 0 | 0 |
| **28** | 0 | 0 | 0 | 0 | 0 | 0 |
| **29** | 0 | 0 | 0 | 0 | 0 | 0 |
| **30** | 0 | 0 | 0 | 0 | 0 | 0 |
| **31** | 0 | 0 | 0 | 0 | 0 | 0 |
| **32** | 0 | 0 | 0 | 0 | 0 | 0 |
| **33** | 0 | 0 | 0 | 0 | 0 | 0 |
| **34** | 0 | 0 | 0 | 0 | 0 | 0 |
| **35** | 0 | 0 | 0 | 0 | 0 | 0 |
| **36** | 0 | 0 | 0 | 0 | 0 | 0 |
| **37** | 0 | 0 | 0 | 0 | 0 | 0 |
| **38** | 0 | 0 | 0 | 0 | 0 | 0 |
| **39** | 0 | 0 | 0 | 0 | 0 | 0 |
| **40** | 0 | 0 | 0 | 0 | 0 | 0 |
| **41** | 0 | 0 | 0 | 0 | 0 | 0 |
| **42** | 0 | 0 | 0 | 0 | 0 | 0 |
| **43** | 0 | 0 | 0 | 0 | 0 | 0 |
| **44** | 0 | 0 | 0 | 0 | 0 | 0 |
| **45** | 0 | 0 | 0 | 0 | 0 | 0 |
| **46** | 0 | 0 | 0 | 0 | 0 | 0 |
| **47** | 0 | 0 | 0 | 0 | 0 | 0 |
| **48** | 0 | 0 | 0 | 0 | 0 | 0 |
| **49** | 0 | 0 | 0 | 0 | 0 | 0 |
| **50** | 0 | 0 | 0 | 0 | 0 | 0 |
| **51** | 0 | 0 | 0 | 0 | 0 | 0 |
| **52** | 0 | 0 | 0 | 0 | 0 | 0 |
| **53** | 0 | 0 | 0 | 0 | 0 | 0 |
| **54** | 0 | 0 | 0 | 0 | 0 | 0 |
| **55** | 0 | 0 | 0 | 0 | 0 | 0 |
| **56** | 0 | 0 | 0 | 0 | 0 | 0 |
| **57** | 0 | 0 | 0 | 0 | 0 | 0 |
| **58** | 0 | 0 | 0 | 0 | 0 | 0 |
| **59** | 0 | 0 | 0 | 0 | 0 | 0 |
| **60** | 0 | 0 | 0 | 0 | 0 | 0 |
| **61** | 0 | 0 | 0 | 0 | 0 | 0 |
| **62** | 0 | 0 | 0 | 0 | 0 | 0 |
| **63** | 0 | 0 | 0 | 0 | 0 | 0 |



Where there is one column per true label and one row per each
         possible weak label

Then we can compute the probability of each weak label given the true
         label by dividing every column by its sum. If we do that, we will get
         a possible estimation of $M_0$

Estimated $M_0$

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **1** | 0.00 | nan | nan | nan | 0.50 | 0.33 |
| **2** | 0.00 | nan | nan | nan | 0.50 | 0.00 |
| **3** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **4** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **5** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **6** | 1.00 | nan | nan | nan | 0.00 | 0.67 |
| **7** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **8** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **9** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **10** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **11** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **12** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **13** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **14** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **15** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **16** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **17** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **18** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **19** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **20** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **21** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **22** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **23** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **24** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **25** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **26** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **27** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **28** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **29** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **30** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **31** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **32** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **33** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **34** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **35** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **36** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **37** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **38** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **39** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **40** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **41** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **42** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **43** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **44** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **45** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **46** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **47** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **48** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **49** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **50** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **51** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **52** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **53** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **54** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **55** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **56** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **57** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **58** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **59** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **60** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **61** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **62** | 0.00 | nan | nan | nan | 0.00 | 0.00 |
| **63** | 0.00 | nan | nan | nan | 0.00 | 0.00 |



However, because given a small data size it is possible that some of
         the weak labels does not occur. We can apply a Laplace correction by
         adding one count to each possible weak label given the true label

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 1 | 1 | 1 | 1 | 1 | 1 |
| **1** | 1 | 1 | 1 | 1 | 2 | 2 |
| **2** | 1 | 1 | 1 | 1 | 2 | 1 |
| **3** | 1 | 1 | 1 | 1 | 1 | 1 |
| **4** | 1 | 1 | 1 | 1 | 1 | 1 |
| **5** | 1 | 1 | 1 | 1 | 1 | 1 |
| **6** | 2 | 1 | 1 | 1 | 1 | 3 |
| **7** | 1 | 1 | 1 | 1 | 1 | 1 |
| **8** | 1 | 1 | 1 | 1 | 1 | 1 |
| **9** | 1 | 1 | 1 | 1 | 1 | 1 |
| **10** | 1 | 1 | 1 | 1 | 1 | 1 |
| **11** | 1 | 1 | 1 | 1 | 1 | 1 |
| **12** | 1 | 1 | 1 | 1 | 1 | 1 |
| **13** | 1 | 1 | 1 | 1 | 1 | 1 |
| **14** | 1 | 1 | 1 | 1 | 1 | 1 |
| **15** | 1 | 1 | 1 | 1 | 1 | 1 |
| **16** | 1 | 1 | 1 | 1 | 1 | 1 |
| **17** | 1 | 1 | 1 | 1 | 1 | 1 |
| **18** | 1 | 1 | 1 | 1 | 1 | 1 |
| **19** | 1 | 1 | 1 | 1 | 1 | 1 |
| **20** | 1 | 1 | 1 | 1 | 1 | 1 |
| **21** | 1 | 1 | 1 | 1 | 1 | 1 |
| **22** | 1 | 1 | 1 | 1 | 1 | 1 |
| **23** | 1 | 1 | 1 | 1 | 1 | 1 |
| **24** | 1 | 1 | 1 | 1 | 1 | 1 |
| **25** | 1 | 1 | 1 | 1 | 1 | 1 |
| **26** | 1 | 1 | 1 | 1 | 1 | 1 |
| **27** | 1 | 1 | 1 | 1 | 1 | 1 |
| **28** | 1 | 1 | 1 | 1 | 1 | 1 |
| **29** | 1 | 1 | 1 | 1 | 1 | 1 |
| **30** | 1 | 1 | 1 | 1 | 1 | 1 |
| **31** | 1 | 1 | 1 | 1 | 1 | 1 |
| **32** | 1 | 1 | 1 | 1 | 1 | 1 |
| **33** | 1 | 1 | 1 | 1 | 1 | 1 |
| **34** | 1 | 1 | 1 | 1 | 1 | 1 |
| **35** | 1 | 1 | 1 | 1 | 1 | 1 |
| **36** | 1 | 1 | 1 | 1 | 1 | 1 |
| **37** | 1 | 1 | 1 | 1 | 1 | 1 |
| **38** | 1 | 1 | 1 | 1 | 1 | 1 |
| **39** | 1 | 1 | 1 | 1 | 1 | 1 |
| **40** | 1 | 1 | 1 | 1 | 1 | 1 |
| **41** | 1 | 1 | 1 | 1 | 1 | 1 |
| **42** | 1 | 1 | 1 | 1 | 1 | 1 |
| **43** | 1 | 1 | 1 | 1 | 1 | 1 |
| **44** | 1 | 1 | 1 | 1 | 1 | 1 |
| **45** | 1 | 1 | 1 | 1 | 1 | 1 |
| **46** | 1 | 1 | 1 | 1 | 1 | 1 |
| **47** | 1 | 1 | 1 | 1 | 1 | 1 |
| **48** | 1 | 1 | 1 | 1 | 1 | 1 |
| **49** | 1 | 1 | 1 | 1 | 1 | 1 |
| **50** | 1 | 1 | 1 | 1 | 1 | 1 |
| **51** | 1 | 1 | 1 | 1 | 1 | 1 |
| **52** | 1 | 1 | 1 | 1 | 1 | 1 |
| **53** | 1 | 1 | 1 | 1 | 1 | 1 |
| **54** | 1 | 1 | 1 | 1 | 1 | 1 |
| **55** | 1 | 1 | 1 | 1 | 1 | 1 |
| **56** | 1 | 1 | 1 | 1 | 1 | 1 |
| **57** | 1 | 1 | 1 | 1 | 1 | 1 |
| **58** | 1 | 1 | 1 | 1 | 1 | 1 |
| **59** | 1 | 1 | 1 | 1 | 1 | 1 |
| **60** | 1 | 1 | 1 | 1 | 1 | 1 |
| **61** | 1 | 1 | 1 | 1 | 1 | 1 |
| **62** | 1 | 1 | 1 | 1 | 1 | 1 |
| **63** | 1 | 1 | 1 | 1 | 1 | 1 |



Estimated $M_0$ with Laplace correction

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **1** | 0.02 | 0.02 | 0.02 | 0.02 | 0.03 | 0.03 |
| **2** | 0.02 | 0.02 | 0.02 | 0.02 | 0.03 | 0.01 |
| **3** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **4** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **5** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **6** | 0.03 | 0.02 | 0.02 | 0.02 | 0.02 | 0.04 |
| **7** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **8** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **9** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **10** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **11** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **12** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **13** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **14** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **15** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **16** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **17** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **18** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **19** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **20** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **21** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **22** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **23** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **24** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **25** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **26** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **27** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **28** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **29** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **30** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **31** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **32** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **33** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **34** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **35** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **36** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **37** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **38** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **39** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **40** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **41** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **42** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **43** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **44** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **45** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **46** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **47** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **48** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **49** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **50** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **51** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **52** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **53** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **54** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **55** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **56** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **57** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **58** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **59** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **60** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **61** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **62** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |
| **63** | 0.02 | 0.02 | 0.02 | 0.02 | 0.02 | 0.01 |



The mixing matrix for the clean data $M_1$

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **1** | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **2** | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 |
| **3** | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 |
| **4** | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 |
| **5** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 |



# ESTIMATION OF THE MIXING MATRIX M FOR ALL DATA ###

Now lets do the same but with the full set of weak and true labels

This is the count

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0 | 0 | 0 | 0 | 0 | 0 |
| **1** | 10 | 5 | 66 | 22 | 165 | 102 |
| **2** | 1 | 0 | 5 | 1 | 31 | 8 |
| **3** | 0 | 0 | 0 | 0 | 0 | 0 |
| **4** | 4 | 3 | 4 | 75 | 17 | 6 |
| **5** | 0 | 0 | 0 | 0 | 0 | 0 |
| **6** | 8 | 29 | 45 | 60 | 486 | 58 |
| **7** | 0 | 0 | 0 | 0 | 0 | 0 |
| **8** | 0 | 0 | 21 | 0 | 1 | 1 |
| **9** | 0 | 0 | 0 | 0 | 0 | 0 |
| **10** | 0 | 0 | 0 | 0 | 0 | 0 |
| **11** | 0 | 0 | 0 | 0 | 0 | 0 |
| **12** | 0 | 0 | 0 | 0 | 0 | 0 |
| **13** | 0 | 0 | 0 | 0 | 0 | 0 |
| **14** | 0 | 0 | 1 | 0 | 0 | 0 |
| **15** | 0 | 0 | 0 | 0 | 0 | 0 |
| **16** | 0 | 1 | 0 | 0 | 0 | 0 |
| **17** | 0 | 0 | 0 | 0 | 0 | 0 |
| **18** | 0 | 1 | 0 | 0 | 0 | 0 |
| **19** | 0 | 0 | 0 | 0 | 0 | 0 |
| **20** | 0 | 0 | 0 | 0 | 0 | 0 |
| **21** | 0 | 0 | 0 | 0 | 0 | 0 |
| **22** | 0 | 0 | 0 | 0 | 0 | 0 |
| **23** | 0 | 0 | 0 | 0 | 0 | 0 |
| **24** | 0 | 0 | 0 | 0 | 0 | 0 |
| **25** | 0 | 0 | 0 | 0 | 0 | 0 |
| **26** | 0 | 0 | 0 | 0 | 0 | 0 |
| **27** | 0 | 0 | 0 | 0 | 0 | 0 |
| **28** | 0 | 0 | 0 | 0 | 0 | 0 |
| **29** | 0 | 0 | 0 | 0 | 0 | 0 |
| **30** | 0 | 0 | 0 | 0 | 0 | 0 |
| **31** | 0 | 0 | 0 | 0 | 0 | 0 |
| **32** | 0 | 0 | 0 | 0 | 0 | 0 |
| **33** | 0 | 0 | 0 | 0 | 0 | 0 |
| **34** | 0 | 0 | 0 | 0 | 0 | 0 |
| **35** | 0 | 0 | 0 | 0 | 0 | 0 |
| **36** | 0 | 0 | 0 | 0 | 0 | 0 |
| **37** | 0 | 0 | 0 | 0 | 0 | 0 |
| **38** | 0 | 0 | 0 | 0 | 0 | 0 |
| **39** | 0 | 0 | 0 | 0 | 0 | 0 |
| **40** | 0 | 0 | 0 | 0 | 0 | 0 |
| **41** | 0 | 0 | 0 | 0 | 0 | 0 |
| **42** | 0 | 0 | 0 | 0 | 0 | 0 |
| **43** | 0 | 0 | 0 | 0 | 0 | 0 |
| **44** | 0 | 0 | 0 | 0 | 0 | 0 |
| **45** | 0 | 0 | 0 | 0 | 0 | 0 |
| **46** | 0 | 0 | 0 | 0 | 0 | 0 |
| **47** | 0 | 0 | 0 | 0 | 0 | 0 |
| **48** | 0 | 0 | 0 | 0 | 0 | 0 |
| **49** | 0 | 0 | 0 | 0 | 0 | 0 |
| **50** | 0 | 0 | 0 | 0 | 0 | 0 |
| **51** | 0 | 0 | 0 | 0 | 0 | 0 |
| **52** | 0 | 0 | 0 | 0 | 0 | 0 |
| **53** | 0 | 0 | 0 | 0 | 0 | 0 |
| **54** | 0 | 0 | 0 | 0 | 0 | 0 |
| **55** | 0 | 0 | 0 | 0 | 0 | 0 |
| **56** | 0 | 0 | 11 | 1 | 0 | 0 |
| **57** | 0 | 0 | 0 | 0 | 0 | 0 |
| **58** | 0 | 0 | 0 | 0 | 0 | 0 |
| **59** | 0 | 0 | 0 | 0 | 0 | 0 |
| **60** | 0 | 0 | 0 | 0 | 0 | 0 |
| **61** | 0 | 0 | 0 | 0 | 0 | 0 |
| **62** | 0 | 0 | 0 | 0 | 0 | 0 |
| **63** | 0 | 0 | 0 | 0 | 0 | 0 |



Estimated $M_0$ without Laplace correction

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **1** | 0.43 | 0.13 | 0.43 | 0.14 | 0.24 | 0.58 |
| **2** | 0.04 | 0.00 | 0.03 | 0.01 | 0.04 | 0.05 |
| **3** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **4** | 0.17 | 0.08 | 0.03 | 0.47 | 0.02 | 0.03 |
| **5** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **6** | 0.35 | 0.74 | 0.29 | 0.38 | 0.69 | 0.33 |
| **7** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **8** | 0.00 | 0.00 | 0.14 | 0.00 | 0.00 | 0.01 |
| **9** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **10** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **11** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **12** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **13** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **14** | 0.00 | 0.00 | 0.01 | 0.00 | 0.00 | 0.00 |
| **15** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **16** | 0.00 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 |
| **17** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **18** | 0.00 | 0.03 | 0.00 | 0.00 | 0.00 | 0.00 |
| **19** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **20** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **21** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **22** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **23** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **24** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **25** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **26** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **27** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **28** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **29** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **30** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **31** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **32** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **33** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **34** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **35** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **36** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **37** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **38** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **39** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **40** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **41** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **42** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **43** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **44** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **45** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **46** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **47** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **48** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **49** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **50** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **51** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **52** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **53** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **54** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **55** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **56** | 0.00 | 0.00 | 0.07 | 0.01 | 0.00 | 0.00 |
| **57** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **58** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **59** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **60** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **61** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **62** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **63** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |



Estimated $M_0$ with Laplace correction

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **1** | 0.13 | 0.06 | 0.31 | 0.10 | 0.22 | 0.43 |
| **2** | 0.02 | 0.01 | 0.03 | 0.01 | 0.04 | 0.04 |
| **3** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **4** | 0.06 | 0.04 | 0.02 | 0.34 | 0.02 | 0.03 |
| **5** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **6** | 0.10 | 0.29 | 0.21 | 0.27 | 0.64 | 0.25 |
| **7** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **8** | 0.01 | 0.01 | 0.10 | 0.00 | 0.00 | 0.01 |
| **9** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **10** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **11** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **12** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **13** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **14** | 0.01 | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 |
| **15** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **16** | 0.01 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 |
| **17** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **18** | 0.01 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 |
| **19** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **20** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **21** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **22** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **23** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **24** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **25** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **26** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **27** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **28** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **29** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **30** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **31** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **32** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **33** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **34** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **35** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **36** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **37** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **38** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **39** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **40** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **41** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **42** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **43** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **44** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **45** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **46** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **47** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **48** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **49** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **50** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **51** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **52** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **53** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **54** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **55** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **56** | 0.01 | 0.01 | 0.06 | 0.01 | 0.00 | 0.00 |
| **57** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **58** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **59** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **60** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **61** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **62** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **63** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |



The mixing matrix for the clean data $M_1$

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **1** | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **2** | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 | 0.00 |
| **3** | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.00 |
| **4** | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 |
| **5** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 |



## COMBINATION OF BOTH MATRICES FOR THE FULL DATASET ###

Proportion of samples with weak ($q_0$) and with true labels ($q_1$)

- $q_0$ = 0.981402897515
- $q_1$ = 0.0185971024851

Composition of mixing matrices $M = [q_0*M_0 \mathtt{ , } q_1*M_1]^T$

| | **0** | **1** | **2** | **3** | **4** | **5** |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **0** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **1** | 0.12 | 0.06 | 0.30 | 0.10 | 0.21 | 0.42 |
| **2** | 0.02 | 0.01 | 0.03 | 0.01 | 0.04 | 0.04 |
| **3** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **4** | 0.06 | 0.04 | 0.02 | 0.33 | 0.02 | 0.03 |
| **5** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **6** | 0.10 | 0.29 | 0.21 | 0.27 | 0.63 | 0.24 |
| **7** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **8** | 0.01 | 0.01 | 0.10 | 0.00 | 0.00 | 0.01 |
| **9** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **10** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **11** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **12** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **13** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **14** | 0.01 | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 |
| **15** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **16** | 0.01 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 |
| **17** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **18** | 0.01 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 |
| **19** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **20** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **21** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **22** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **23** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **24** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **25** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **26** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **27** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **28** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **29** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **30** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **31** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **32** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **33** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **34** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **35** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **36** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **37** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **38** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **39** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **40** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **41** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **42** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **43** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **44** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **45** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **46** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **47** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **48** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **49** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **50** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **51** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **52** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **53** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **54** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **55** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **56** | 0.01 | 0.01 | 0.05 | 0.01 | 0.00 | 0.00 |
| **57** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **58** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **59** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **60** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **61** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **62** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **63** | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0.00 |
| **64** | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| **65** | 0.00 | 0.02 | 0.00 | 0.00 | 0.00 | 0.00 |
| **66** | 0.00 | 0.00 | 0.02 | 0.00 | 0.00 | 0.00 |
| **67** | 0.00 | 0.00 | 0.00 | 0.02 | 0.00 | 0.00 |
| **68** | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 | 0.00 |
| **69** | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.02 |



The corresponding indices of the weak labels to the rows of the matrix M

<pre>
[1 6 6 2 6 1]
</pre>


The corresponding indices of the true labels to the rows of the matrix M

<pre>
[68 69 69 68 64 69]
</pre>


# TRAINING SOME MODELS ###

From here I will add an extra feature fixed to 1 for the bias

<pre>
[[  0.00e+00   3.55e+02   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  1.20e+01   4.42e+03   3.50e+01 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   4.50e+01   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]]
</pre>


## SIMULATION OF GRADIENT DESCENT FOR A LR ###

Initial weights w

<pre>
[[ 1.76  0.4   0.98  2.24  1.87 -0.98]
 [ 0.95 -0.15 -0.1   0.41  0.14  1.45]
 [ 0.76  0.12  0.44  0.33  1.49 -0.21]
 ..., 
 [-0.46 -0.83 -0.26 -0.72 -0.   -0.28]
 [ 1.83 -0.27  1.59 -0.6  -0.19 -0.25]
 [-0.17 -1.21 -0.45  0.38  0.31  0.37]]
</pre>


The small sample of X

<pre>
[[  0.00e+00   3.55e+02   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  1.20e+01   4.42e+03   3.50e+01 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   4.50e+01   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]]
</pre>


Initial model activations m_a = X m_W

<pre>
[[  1.64e+02  -2.29e+01  -3.21e+02   5.45e+01  -2.16e+00   2.88e+02]
 [ -4.34e+02  -7.09e+03  -2.20e+04   1.23e+04   4.67e+03  -5.84e+03]
 [ -2.06e+00  -4.33e+00  -2.15e+00   7.56e-01   5.78e-01   1.76e+00]
 [  9.44e+01  -2.05e+02  -2.03e+02   1.04e+02   1.87e+02  -7.98e+01]
 [ -1.68e-01  -1.21e+00  -4.48e-01   3.81e-01   3.10e-01   3.67e-01]
 [ -1.68e-01  -1.21e+00  -4.48e-01   3.81e-01   3.10e-01   3.67e-01]]
</pre>


Initial model outputs m_q = softmax(m_a)

<pre>
[[  7.60e-055   6.60e-136   1.74e-265   2.79e-102   6.80e-127   1.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   1.00e+000   0.00e+000   0.00e+000]
 [  1.27e-002   1.32e-003   1.17e-002   2.13e-001   1.79e-001   5.82e-001]
 [  4.71e-041   5.43e-171   5.26e-170   7.37e-037   1.00e+000   1.01e-116]
 [  1.40e-001   4.90e-002   1.06e-001   2.42e-001   2.25e-001   2.39e-001]
 [  1.40e-001   4.90e-002   1.06e-001   2.42e-001   2.25e-001   2.39e-001]]
</pre>


Initial model predictions m_pred = argmax(m_q)

<pre>
[5 3 5 4 3 3]
</pre>


Compute the error of the predicted probabilities q true labels Y

<pre>
[  2.00e+00   2.00e+00   2.52e-01   5.43e-73   9.20e-01   7.22e-01]
</pre>


Compute the gradient of the Brier score

<pre>
[[  0.00e+000   0.00e+000   0.00e+000   1.20e+001   0.00e+000  -1.20e+001]
 [  2.12e-039   2.34e-133   2.37e-168   4.42e+003  -3.55e+002  -4.06e+003]
 [  0.00e+000   0.00e+000   0.00e+000   3.50e+001   0.00e+000  -3.50e+001]
 ..., 
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000]
 [ -7.08e-001   9.94e-002   2.23e-001   1.70e+000  -3.71e-001  -9.41e-001]]
</pre>


Update the weights $W_{t+1} = W_t - G$

<pre>
[[  1.76e+00   4.00e-01   9.79e-01  -9.76e+00   1.87e+00   1.10e+01]
 [  9.50e-01  -1.51e-01  -1.03e-01  -4.42e+03   3.55e+02   4.07e+03]
 [  7.61e-01   1.22e-01   4.44e-01  -3.47e+01   1.49e+00   3.48e+01]
 ..., 
 [ -4.59e-01  -8.28e-01  -2.63e-01  -7.19e-01  -4.61e-03  -2.80e-01]
 [  1.83e+00  -2.70e-01   1.59e+00  -6.04e-01  -1.86e-01  -2.49e-01]
 [  5.40e-01  -1.31e+00  -6.71e-01  -1.32e+00   6.81e-01   1.31e+00]]
</pre>


### WE CAN PERFORM GD FOR SOME ITERATIONS FULL TRUE SET ###

Iterations :

| it | mean bs | acc |
| :--- | :-----------: | :-----: |
| 0 | 1.71E+00 | 0.14 |
| 1 | 1.59E+00 | 0.20 |
| 2 | 8.78E-01 | 0.56 |
| 3 | 1.70E+00 | 0.15 |
| 4 | 1.51E+00 | 0.24 |
| 5 | 1.45E+00 | 0.28 |
| 6 | 1.32E+00 | 0.34 |
| 7 | 9.62E-01 | 0.52 |
| 8 | 1.63E+00 | 0.18 |
| 9 | 8.44E-01 | 0.58 |
| 10 | 1.50E+00 | 0.25 |
| 11 | 9.54E-01 | 0.52 |
| 12 | 1.38E+00 | 0.31 |
| 13 | 8.15E-01 | 0.59 |
| 14 | 1.58E+00 | 0.21 |
| 15 | 8.33E-01 | 0.58 |
| 16 | 1.64E+00 | 0.18 |
| 17 | 1.51E+00 | 0.25 |
| 18 | 1.38E+00 | 0.31 |
| 19 | 8.42E-01 | 0.58 |

## SIMULATION OF THE EXPECTATION MAXIMIZATION STEPS FOR A LR ###

Initial weights w

<pre>
[[ 1.76  0.4   0.98  2.24  1.87 -0.98]
 [ 0.95 -0.15 -0.1   0.41  0.14  1.45]
 [ 0.76  0.12  0.44  0.33  1.49 -0.21]
 ..., 
 [-0.46 -0.83 -0.26 -0.72 -0.   -0.28]
 [ 1.83 -0.27  1.59 -0.6  -0.19 -0.25]
 [-0.17 -1.21 -0.45  0.38  0.31  0.37]]
</pre>


The small sample of X

<pre>
[[  0.00e+00   3.55e+02   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  1.20e+01   4.42e+03   3.50e+01 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   4.50e+01   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]]
</pre>


#### EXPECTATION STEP: Assume the weights of the model are right ###

Initial prediction Z = X w

<pre>
[[  1.64e+02  -2.29e+01  -3.21e+02   5.45e+01  -2.16e+00   2.88e+02]
 [ -4.34e+02  -7.09e+03  -2.20e+04   1.23e+04   4.67e+03  -5.84e+03]
 [ -2.06e+00  -4.33e+00  -2.15e+00   7.56e-01   5.78e-01   1.76e+00]
 [  9.44e+01  -2.05e+02  -2.03e+02   1.04e+02   1.87e+02  -7.98e+01]
 [ -1.68e-01  -1.21e+00  -4.48e-01   3.81e-01   3.10e-01   3.67e-01]
 [ -1.68e-01  -1.21e+00  -4.48e-01   3.81e-01   3.10e-01   3.67e-01]]
</pre>


Initial q = softmax(Z)

<pre>
[[  7.60e-055   6.60e-136   1.74e-265   2.79e-102   6.80e-127   1.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   1.00e+000   0.00e+000   0.00e+000]
 [  1.27e-002   1.32e-003   1.17e-002   2.13e-001   1.79e-001   5.82e-001]
 [  4.71e-041   5.43e-171   5.26e-170   7.37e-037   1.00e+000   1.01e-116]
 [  1.40e-001   4.90e-002   1.06e-001   2.42e-001   2.25e-001   2.39e-001]
 [  1.40e-001   4.90e-002   1.06e-001   2.42e-001   2.25e-001   2.39e-001]]
</pre>


Initial predictions

<pre>
[5 3 5 4 3 3]
</pre>


Compute the virtual labels Q = q * M[Z_index]

<pre>
[[  9.43e-056   3.77e-137   5.28e-266   2.82e-103   1.45e-127   4.23e-001]
 [  0.00e+000   0.00e+000   0.00e+000   2.68e-001   0.00e+000   0.00e+000]
 [  1.29e-003   3.78e-004   2.43e-003   5.73e-002   1.12e-001   1.41e-001]
 [  1.06e-042   5.18e-173   1.43e-171   6.48e-039   4.11e-002   3.72e-118]
 [  1.42e-002   1.40e-002   2.20e-002   6.50e-002   1.41e-001   5.78e-002]
 [  1.73e-002   2.80e-003   3.20e-002   2.45e-002   4.80e-002   1.01e-001]]
</pre>


Normalize the virtual labels to sum to one

<pre>
[[  2.23e-055   8.92e-137   1.25e-265   6.67e-103   3.43e-127   1.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   1.00e+000   0.00e+000   0.00e+000]
 [  4.12e-003   1.20e-003   7.74e-003   1.82e-001   3.55e-001   4.49e-001]
 [  2.58e-041   1.26e-171   3.47e-170   1.58e-037   1.00e+000   9.06e-117]
 [  4.52e-002   4.47e-002   7.00e-002   2.07e-001   4.49e-001   1.84e-001]
 [  7.68e-002   1.24e-002   1.42e-001   1.09e-001   2.13e-001   4.47e-001]]
</pre>


Search for infinite or NaN values and remove them for this training step
The following samples are NOT removed from the training

<pre>
[0 1 2 3 4 5]
</pre>


#### MAXIMIZATION STEP: In our case update the weights to MINIMIZE the error###

Compute the error of the predicted probabilities q against the new Virtual labels Q

<pre>
[  2.88e-109   0.00e+000   5.01e-002   3.35e-073   6.45e-002   6.81e-002]
</pre>


Compute the gradient of the Brier score

<pre>
[[  0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000]
 [  4.50e-080  -2.00e-241  -1.01e-240   1.92e-071  -1.92e-071  -1.45e-106]
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000]
 ..., 
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000]
 [  2.42e-002   2.91e-003   1.45e-003   4.09e-002  -8.42e-002   1.47e-002]]
</pre>


Update the weights $W_{t+1} = W_t - G$

<pre>
[[ 1.76  0.4   0.98  2.24  1.87 -0.98]
 [ 0.95 -0.15 -0.1   0.41  0.14  1.45]
 [ 0.76  0.12  0.44  0.33  1.49 -0.21]
 ..., 
 [-0.46 -0.83 -0.26 -0.72 -0.   -0.28]
 [ 1.83 -0.27  1.59 -0.6  -0.19 -0.25]
 [-0.19 -1.22 -0.45  0.34  0.39  0.35]]
</pre>


### WE CAN PERFORM EM FOR SOME ITERATIONS ###

Iterations :

| it | mean bs | acc |
| :-- | :-------: | :---: |
| 0 | 1.07E-02 | 0.25
| 1 | 8.16E-05 | 0.56
| 2 | 2.91E-05 | 0.56
| 3 | 2.40E-05 | 0.56
| 4 | 2.09E-05 | 0.56
| 5 | 1.90E-05 | 0.56
| 6 | 1.77E-05 | 0.56
| 7 | 1.70E-05 | 0.56
| 8 | 1.71E-05 | 0.56
| 9 | 2.00E-05 | 0.56
| 10 | 3.79E-05 | 0.56
| 11 | 5.06E-05 | 0.56
| 12 | 1.22E-05 | 0.56
| 13 | 1.18E-05 | 0.56
| 14 | 1.14E-05 | 0.56
| 15 | 1.11E-05 | 0.56
| 16 | 1.08E-05 | 0.56
| 17 | 1.05E-05 | 0.56
| 18 | 1.02E-05 | 0.56
| 19 | 1.00E-05 | 0.56
