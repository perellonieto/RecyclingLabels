Most of the following floats are limited to a precision of 2 decimals

# DATA DESCRIPTION ###

- name = webs
- Number of features = 2099
- Number of classes = 6
- Classes = ['blog', 'inmo', 'parking', 'b2c', 'no_b2c', 'Other']
- Samples with only weak = 65912
- Samples with true labels = 1249

## DATA SAMPLE ###

Example of true labels Y

- In decimal y =

<pre>
[4 4 3 3 2 4]
</pre>


- In binary Y =

<pre>
[[0 0 0 0 1 0]
 [0 0 0 0 1 0]
 [0 0 0 1 0 0]
 [0 0 0 1 0 0]
 [0 0 1 0 0 0]
 [0 0 0 0 1 0]]
</pre>


Example of corresponding weak labels Z

- In decimal z =

<pre>
[6 6 6 4 1 6]
</pre>


- In binary Z =

<pre>
[[0 0 0 1 1 0]
 [0 0 0 1 1 0]
 [0 0 0 1 1 0]
 [0 0 0 1 0 0]
 [0 0 0 0 0 1]
 [0 0 0 1 1 0]]
</pre>


Prior distribution for **all** true labels. $P(Y)$

<pre>
[ 0.02  0.03  0.12  0.13  0.56  0.14]
</pre>


# EXPECTED ERROR FOR A BASELINE ###

We show the performance of a simple model that always predicts the prior

## BRIER SCORE #
Error matrix with Brier score: $\Psi_{BS} = BS(P(Y), I)$

<pre>
[[  9.64e-01   9.75e-04   1.50e-02   1.62e-02   3.14e-01   1.96e-02]
 [  3.39e-04   9.39e-01   1.50e-02   1.62e-02   3.14e-01   1.96e-02]
 [  3.39e-04   9.75e-04   7.70e-01   1.62e-02   3.14e-01   1.96e-02]
 [  3.39e-04   9.75e-04   1.50e-02   7.62e-01   3.14e-01   1.96e-02]
 [  3.39e-04   9.75e-04   1.50e-02   1.62e-02   1.93e-01   1.96e-02]
 [  3.39e-04   9.75e-04   1.50e-02   1.62e-02   3.14e-01   7.39e-01]]
</pre>


Expected Brier score: $\mathbb{E}_{y\sim P(y)} [\Psi_{BS}(S, y)] = \sum_{j=1}^K P(y=j) \Psi_{BS}(S, y_j)$

<pre>
0.633740619397
</pre>


## LOG-LOSS #
Error matrix with Log-loss: $\Psi_{LL} = LL(P(Y), I)$

<pre>
[[ 3.99 -0.   -0.   -0.   -0.   -0.  ]
 [-0.    3.47 -0.   -0.   -0.   -0.  ]
 [-0.   -0.    2.1  -0.   -0.   -0.  ]
 [-0.   -0.   -0.    2.06 -0.   -0.  ]
 [-0.   -0.   -0.   -0.    0.58 -0.  ]
 [-0.   -0.   -0.   -0.   -0.    1.97]]
</pre>


Expected Log-loss: $\mathbb{E}_{y\sim P(y)} [\Psi_{LL}(S, y)] = \sum_{j=1}^K P(y=j) \Psi_{LL}(S, y_j)$

<pre>
1.30127399351
</pre>


# ESTIMATION OF THE MIXING MATRIX M ###
From now, $M_0$ is for the weak set and $M_1$ for the **full set** that contains the true labels

Estimated $M_0$ without Laplace correction

<pre>
[[ 0.    0.    0.    0.    0.    0.  ]
 [ 0.43  0.13  0.43  0.14  0.24  0.58]
 [ 0.04  0.    0.03  0.01  0.04  0.05]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.17  0.08  0.03  0.47  0.02  0.03]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.35  0.74  0.29  0.38  0.69  0.33]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.14  0.    0.    0.01]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.01  0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.03  0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.03  0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.07  0.01  0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]
 [ 0.    0.    0.    0.    0.    0.  ]]
</pre>


Estimated $M_0$ with Laplace correction

<pre>
[[ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.13  0.06  0.31  0.1   0.22  0.43]
 [ 0.02  0.01  0.03  0.01  0.04  0.04]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.06  0.04  0.02  0.34  0.02  0.03]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.1   0.29  0.21  0.27  0.64  0.25]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.1   0.    0.    0.01]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.01  0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.02  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.02  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.06  0.01  0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]]
</pre>


The mixing matrix for the clean data $M_1$

<pre>
[[ 1.  0.  0.  0.  0.  0.]
 [ 0.  1.  0.  0.  0.  0.]
 [ 0.  0.  1.  0.  0.  0.]
 [ 0.  0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0.  1.  0.]
 [ 0.  0.  0.  0.  0.  1.]]
</pre>


## COMBINATION OF BOTH MATRICES FOR THE FULL DATASET ###

Proportion of samples with weak ($q_0$) and with true labels ($q_1$)

- $q_0$ = 0.981402897515
- $q_1$ = 0.0185971024851

Composition of mixing matrices $M = [q_0*M_0 \mathtt{ , } q_1*M_1]^T$

<pre>
[[ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.12  0.06  0.3   0.1   0.21  0.42]
 [ 0.02  0.01  0.03  0.01  0.04  0.04]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.06  0.04  0.02  0.33  0.02  0.03]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.1   0.29  0.21  0.27  0.63  0.24]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.1   0.    0.    0.01]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.01  0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.02  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.02  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.05  0.01  0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.01  0.01  0.    0.    0.    0.  ]
 [ 0.02  0.    0.    0.    0.    0.  ]
 [ 0.    0.02  0.    0.    0.    0.  ]
 [ 0.    0.    0.02  0.    0.    0.  ]
 [ 0.    0.    0.    0.02  0.    0.  ]
 [ 0.    0.    0.    0.    0.02  0.  ]
 [ 0.    0.    0.    0.    0.    0.02]]
</pre>


The corresponding indices of the weak labels to the rows of the matrix M

<pre>
[6 6 6 4 1 6]
</pre>


The corresponding indices of the true labels to the rows of the matrix M

<pre>
[68 68 67 67 66 68]
</pre>


# TRAINING SOME MODELS ###

From here I will add an extra feature fixed to 1 for the bias

<pre>
[[  3.80e+01   2.16e+02   1.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   9.90e+01   2.00e+01 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  6.30e+02   4.97e+03   6.30e+02 ...,   1.70e+01   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  5.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  1.30e+01   5.70e+01   5.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]]
</pre>


## SIMULATION OF GRADIENT DESCENT FOR A LR ###

Initial weights w

<pre>
[[ 1.76  0.4   0.98  2.24  1.87 -0.98]
 [ 0.95 -0.15 -0.1   0.41  0.14  1.45]
 [ 0.76  0.12  0.44  0.33  1.49 -0.21]
 ..., 
 [-0.46 -0.83 -0.26 -0.72 -0.   -0.28]
 [ 1.83 -0.27  1.59 -0.6  -0.19 -0.25]
 [-0.17 -1.21 -0.45  0.38  0.31  0.37]]
</pre>


The small sample of X

<pre>
[[  3.80e+01   2.16e+02   1.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   9.90e+01   2.00e+01 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  6.30e+02   4.97e+03   6.30e+02 ...,   1.70e+01   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  5.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  1.30e+01   5.70e+01   5.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]]
</pre>


Initial model activations m_a = X m_W

<pre>
[[  1.53e+02  -1.20e+03  -2.20e+03   1.28e+03   8.41e+02  -1.82e+03]
 [ -5.68e+02   1.61e+03  -1.76e+03   1.25e+03   1.53e+03  -1.45e+03]
 [  2.26e+04  -1.07e+04  -3.06e+04  -1.87e+04   4.58e+04  -4.18e+04]
 [  1.18e+02   1.59e+02  -2.30e+01  -7.17e+01  -6.29e+01  -6.03e+01]
 [  6.23e+01  -1.62e+01   1.10e+01   3.11e+00   6.91e+01  -2.90e+00]
 [ -1.89e+02   2.76e+01  -4.72e+02   1.01e+02   2.53e+02  -3.37e+02]]
</pre>


Initial model outputs m_q = softmax(m_a)

<pre>
[[  0.00e+000   0.00e+000   0.00e+000   1.00e+000   2.10e-191   0.00e+000]
 [  0.00e+000   1.00e+000   0.00e+000   7.22e-156   1.11e-036   0.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   1.00e+000   0.00e+000]
 [  1.15e-018   1.00e+000   9.52e-080   6.60e-101   4.51e-097   5.80e-096]
 [  1.10e-003   9.56e-038   5.76e-026   2.25e-029   9.99e-001   5.49e-032]
 [  1.45e-192   1.07e-098   1.56e-315   9.81e-067   1.00e+000   4.00e-257]]
</pre>


Initial model predictions m_pred = argmax(m_q)

<pre>
[3 1 4 1 4 4]
</pre>


Compute the error of the predicted probabilities q true labels Y

<pre>
[  2.00e+000   2.00e+000   2.00e+000   2.00e+000   2.00e+000   9.62e-133]
</pre>


Compute the gradient of the Brier score

<pre>
[[  5.52e-003   4.78e-037  -5.00e+000  -5.92e+002   5.97e+002   2.75e-031]
 [  8.27e-191   9.90e+001   8.87e-314  -4.76e+003   4.66e+003   2.28e-255]
 [  7.26e-192   2.00e+001   7.78e-315  -6.29e+002   6.09e+002   2.00e-256]
 ..., 
 [  0.00e+000   0.00e+000   0.00e+000  -1.70e+001   1.70e+001   0.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000]
 [  1.10e-003   2.00e+000  -1.00e+000  -1.00e+000  -1.10e-003   5.49e-032]]
</pre>


Update the weights $W_{t+1} = W_t - G$

<pre>
[[  1.76e+00   4.00e-01   5.98e+00   5.94e+02  -5.95e+02  -9.77e-01]
 [  9.50e-01  -9.92e+01  -1.03e-01   4.76e+03  -4.66e+03   1.45e+00]
 [  7.61e-01  -1.99e+01   4.44e-01   6.29e+02  -6.08e+02  -2.05e-01]
 ..., 
 [ -4.59e-01  -8.28e-01  -2.63e-01   1.63e+01  -1.70e+01  -2.80e-01]
 [  1.83e+00  -2.70e-01   1.59e+00  -6.04e-01  -1.86e-01  -2.49e-01]
 [ -1.70e-01  -3.21e+00   5.52e-01   1.38e+00   3.11e-01   3.67e-01]]
</pre>


### WE CAN PERFORM GD FOR SOME ITERATIONS FULL TRUE SET ###

Iterations :

| it | mean bs | acc |
| :--- | :-----------: | :-----: |
| 0 | 1.70E+00 | 0.13 |
| 1 | 1.26E+00 | 0.37 |
| 2 | 8.42E-01 | 0.58 |
| 3 | 1.68E+00 | 0.16 |
| 4 | 1.51E+00 | 0.25 |
| 5 | 1.51E+00 | 0.25 |
| 6 | 8.57E-01 | 0.57 |
| 7 | 1.62E+00 | 0.19 |
| 8 | 1.44E+00 | 0.28 |
| 9 | 1.29E+00 | 0.36 |
| 10 | 8.84E-01 | 0.56 |
| 11 | 1.54E+00 | 0.23 |
| 12 | 8.09E-01 | 0.60 |
| 13 | 1.61E+00 | 0.19 |
| 14 | 1.38E+00 | 0.31 |
| 15 | 1.18E+00 | 0.41 |
| 16 | 7.81E-01 | 0.61 |
| 17 | 1.51E+00 | 0.24 |
| 18 | 8.69E-01 | 0.57 |
| 19 | 1.53E+00 | 0.24 |

## SIMULATION OF THE EXPECTATION MAXIMIZATION STEPS FOR A LR ###

Initial weights w

<pre>
[[ 1.76  0.4   0.98  2.24  1.87 -0.98]
 [ 0.95 -0.15 -0.1   0.41  0.14  1.45]
 [ 0.76  0.12  0.44  0.33  1.49 -0.21]
 ..., 
 [-0.46 -0.83 -0.26 -0.72 -0.   -0.28]
 [ 1.83 -0.27  1.59 -0.6  -0.19 -0.25]
 [-0.17 -1.21 -0.45  0.38  0.31  0.37]]
</pre>


The small sample of X

<pre>
[[  3.80e+01   2.16e+02   1.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  0.00e+00   9.90e+01   2.00e+01 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  6.30e+02   4.97e+03   6.30e+02 ...,   1.70e+01   0.00e+00   1.00e+00]
 [  0.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  5.00e+00   0.00e+00   0.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]
 [  1.30e+01   5.70e+01   5.00e+00 ...,   0.00e+00   0.00e+00   1.00e+00]]
</pre>


#### EXPECTATION STEP: Assume the weights of the model are right ###

Initial prediction Z = X w

<pre>
[[  1.53e+02  -1.20e+03  -2.20e+03   1.28e+03   8.41e+02  -1.82e+03]
 [ -5.68e+02   1.61e+03  -1.76e+03   1.25e+03   1.53e+03  -1.45e+03]
 [  2.26e+04  -1.07e+04  -3.06e+04  -1.87e+04   4.58e+04  -4.18e+04]
 [  1.18e+02   1.59e+02  -2.30e+01  -7.17e+01  -6.29e+01  -6.03e+01]
 [  6.23e+01  -1.62e+01   1.10e+01   3.11e+00   6.91e+01  -2.90e+00]
 [ -1.89e+02   2.76e+01  -4.72e+02   1.01e+02   2.53e+02  -3.37e+02]]
</pre>


Initial q = softmax(Z)

<pre>
[[  0.00e+000   0.00e+000   0.00e+000   1.00e+000   2.10e-191   0.00e+000]
 [  0.00e+000   1.00e+000   0.00e+000   7.22e-156   1.11e-036   0.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   1.00e+000   0.00e+000]
 [  1.15e-018   1.00e+000   9.52e-080   6.60e-101   4.51e-097   5.80e-096]
 [  1.10e-003   9.56e-038   5.76e-026   2.25e-029   9.99e-001   5.49e-032]
 [  1.45e-192   1.07e-098   1.56e-315   9.81e-067   1.00e+000   4.00e-257]]
</pre>


Initial predictions

<pre>
[3 1 4 1 4 4]
</pre>


Compute the virtual labels Q = q * M[Z_index]

<pre>
[[  0.00e+000   0.00e+000   0.00e+000   2.68e-001   1.32e-191   0.00e+000]
 [  0.00e+000   2.86e-001   0.00e+000   1.94e-156   6.92e-037   0.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   6.26e-001   0.00e+000]
 [  6.50e-020   3.81e-002   2.15e-081   2.21e-101   1.04e-098   1.67e-097]
 [  1.37e-004   5.46e-039   1.74e-026   2.28e-030   2.13e-001   2.32e-032]
 [  1.47e-193   3.07e-099   3.24e-316   2.63e-067   6.26e-001   9.70e-258]]
</pre>


Normalize the virtual labels to sum to one

<pre>
[[  0.00e+000   0.00e+000   0.00e+000   1.00e+000   4.90e-191   0.00e+000]
 [  0.00e+000   1.00e+000   0.00e+000   6.78e-156   2.42e-036   0.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   1.00e+000   0.00e+000]
 [  1.71e-018   1.00e+000   5.65e-080   5.79e-100   2.73e-097   4.37e-096]
 [  6.43e-004   2.56e-038   8.18e-026   1.07e-029   9.99e-001   1.09e-031]
 [  2.36e-193   4.91e-099   5.17e-316   4.21e-067   1.00e+000   1.55e-257]]
</pre>


Search for infinite or NaN values and remove them for this training step
The following samples are NOT removed from the training

<pre>
[0 1 2 3 4 5]
</pre>


#### MAXIMIZATION STEP: In our case update the weights to MINIMIZE the error###

Compute the error of the predicted probabilities q against the new Virtual labels Q

<pre>
[  0.00e+000   1.73e-072   0.00e+000   3.06e-037   4.26e-007   3.13e-133]
</pre>


Compute the gradient of the Brier score

<pre>
[[  5.09e-006   2.20e-040   1.32e-028   5.18e-032  -5.09e-006   1.26e-034]
 [  0.00e+000   1.44e-070   0.00e+000   3.13e-131  -1.44e-070   0.00e+000]
 [  0.00e+000   2.90e-071   0.00e+000   2.74e-132  -2.90e-071   0.00e+000]
 ..., 
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000]
 [  0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000   0.00e+000]
 [  1.02e-006   6.37e-037   2.65e-029   1.04e-032  -1.02e-006   2.53e-035]]
</pre>


Update the weights $W_{t+1} = W_t - G$

<pre>
[[ 1.76  0.4   0.98  2.24  1.87 -0.98]
 [ 0.95 -0.15 -0.1   0.41  0.14  1.45]
 [ 0.76  0.12  0.44  0.33  1.49 -0.21]
 ..., 
 [-0.46 -0.83 -0.26 -0.72 -0.   -0.28]
 [ 1.83 -0.27  1.59 -0.6  -0.19 -0.25]
 [-0.17 -1.21 -0.45  0.38  0.31  0.37]]
</pre>


### WE CAN PERFORM EM FOR SOME ITERATIONS ###

Iterations :

| it | mean bs | acc |
| :-- | :-------: | :---: |
| 0 | 1.22E-02 | 0.24
| 1 | 1.12E-03 | 0.46
| 2 | 1.58E-04 | 0.56
| 3 | 1.18E-04 | 0.56
| 4 | 1.37E-04 | 0.56
| 5 | 4.58E-05 | 0.56
| 6 | 3.75E-05 | 0.56
| 7 | 3.50E-05 | 0.56
| 8 | 3.30E-05 | 0.56
| 9 | 3.13E-05 | 0.56
| 10 | 2.99E-05 | 0.56
| 11 | 2.87E-05 | 0.56
| 12 | 2.77E-05 | 0.56
| 13 | 2.68E-05 | 0.56
| 14 | 2.60E-05 | 0.56
| 15 | 2.53E-05 | 0.56
| 16 | 2.46E-05 | 0.56
| 17 | 2.41E-05 | 0.56
| 18 | 2.35E-05 | 0.56
| 19 | 2.31E-05 | 0.56
