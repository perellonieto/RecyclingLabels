{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# A point analysis of weak learning models\n",
    "\n",
    "    Notebook version: 1.2 (May 29, 2017)\n",
    "\n",
    "    Author: Jesús Cid Sueiro (jcid@tsc.uc3m.es)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "    Changes: v.1.0 - First version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# To visualize plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Import some libraries that will be necessary for working with data and displaying plots\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "# Local imports\n",
    "import wlc.WLweakener as wlw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## 1. Introduction.\n",
    "\n",
    "In this notebook we analyze the behavior of sample based estimates of a $C$-dimensional probability vector $\\boldsymbol{\\eta}$ from binary vector instances.\n",
    "\n",
    "The main goal is to compare the behavior of a \"supervised\" estimate, based on a set of labels $\\{{\\bf y}_k, k=0,\\ldots, K-1\\}$ generated from $\\boldsymbol{\\eta}$, and estimates based on weak labels, $\\{{\\bf b}_k, k=0,\\ldots, K-1\\}$, generated from some related distribution ${\\bf q} = {\\bf M}\\boldsymbol{\\eta}$, where ${\\bf M}$ is called a mixing matrix and contains conditional probabilities.\n",
    "\n",
    "To do so, we will generate a dataset of labels drawn from $\\boldsymbol{\\eta}$, and one or more datasets of weak labels drawn from ${\\bf q}$.\n",
    "\n",
    "First we define some configurable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONFIGURABLE PARAMETERS FOR THIS SECTION\n",
    "C = 3                             # Number of classes\n",
    "K = 10000                         # Sample size\n",
    "eta = np.array([0.5, 0.2, 0.3])   # True probability vector\n",
    "\n",
    "# Weak Label model. \n",
    "# This is the model used to generate the data.\n",
    "# Available options are: quasi_IPL, noisy, random_noise ...\n",
    "wl_model = 'quasi_IPL'  #  'random_noise'   # 'noisy', 'quasi_IPL'\n",
    "\n",
    "# Parameters of the weak label model\n",
    "beta = 0.8\n",
    "\n",
    "# Virtual label model\n",
    "# This it the model used to estimate the probability vector\n",
    "# Available options are: quasi_IPL, noisy, Mproper, ...\n",
    "vl_model = 'Mproper'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Mixing Matrix.\n",
    "\n",
    "Based on the selected weak label model, we generate and visualize the corresponding mixing matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mixing matrix\n",
    "M = wlw.computeM(C, beta=beta, method=wl_model)\n",
    "\n",
    "if wl_model == 'quasi_IPL':\n",
    "    dec_labels = np.arange(0, 2**C)\n",
    "elif wl_model in ['noisy', 'random_noise']:\n",
    "    dec_labels = 2**np.arange(C-1, -1, -1)\n",
    "else:\n",
    "    raise ValueError(\"dec_labels not implemented for the given wl_model\")\n",
    "\n",
    "# Remove zero rows\n",
    "flag = np.nonzero(np.sum(M, axis=1))[0]\n",
    "M = M[flag, :]                 # This is to remove zero rows, which are not relevant\n",
    "dec_labels = dec_labels[flag]  # This is to remove zero rows, which are not relevant\n",
    "\n",
    "# Show:\n",
    "print(\"Mixing matrix:\")\n",
    "print(M)\n",
    "print(\"Corresponding to weak label indices {0}\".format(dec_labels))\n",
    "\n",
    "plt.imshow(M, interpolation='nearest', cmap=\"gray\", clim=(0.0, 1.0)) \n",
    "plt.colorbar()\n",
    "plt.title('Mixing Matrix')\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Weak label')\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataset generation.\n",
    "\n",
    "In the following we will generate a dataset of labels and their corresponding weak labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate true labels\n",
    "I = np.eye(C)\n",
    "iy = np.random.choice(np.arange(0, C), size=K, p=eta)\n",
    "y = I[iy]\n",
    "\n",
    "# Generate weak label indices\n",
    "z = wlw.generateWeak(iy, M, dec_labels)\n",
    "print z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Supervised, sample based estimation.\n",
    "\n",
    "In the supervised setting, ${\\bf y}$ is observed an the optimal sample based estimate of $\\boldsymbol{\\eta}$ (minimizing any Bregman divergence) based on the observed labels is the sample average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = np.mean(y, axis=0)\n",
    "print \"Supervised estimate: {0}\".format(f)\n",
    "e2 = np.sum((f - eta)**2)\n",
    "print \"Square error: {0}\".format(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Learning from weak labels.\n",
    "\n",
    "There are many ways to estimate $\\boldsymbol{\\eta}$ from the weak labels. We consider here a representative sample of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1. Averaging virtual labels\n",
    "\n",
    "If ${\\bf z}$ is a sample from distribution ${\\bf q}$, and ${\\bf V}$ is any left inverse of the mixing matrix (so that ${\\bf V}{\\bf M} = {\\bf I}$ then it can be shown that $\\mathbb{E}\\{{\\bf v}\\} = \\boldsymbol{\\eta}$. Therefore, we can estimate $\\boldsymbol{\\eta}$ as the average of virtual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = wlw.computeVirtual(z, C, method=vl_model, M=M, dec_labels=dec_labels)\n",
    "\n",
    "print \"Virtual labels are:\"\n",
    "print v\n",
    "f_v = np.mean(v, axis=0)\n",
    "\n",
    "print \"Virtual label estimate: {0}\".format(f_v)\n",
    "e2 = np.sum((f_v - eta)**2)\n",
    "print \"Square error: {0}\".format(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of ${\\bf V}$ is relevant. Different left inverses of the mixing matrix may have better behavior under finite samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2. Maximum Likelihood Estimate\n",
    "\n",
    "The expected value of a virtual label vector can be shown to be equal to the minimizer of the expected log likelihood. This implies that, on average, the average of the virtual label vector and the ML estimate should be assymptotically equivalent. However, for a finite sample size, they can also lead to different results.\n",
    "\n",
    "The following function computes the ML estimate by means of the EM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeML(iz, M, f0=None, max_iter=1e10, echo='off'):\n",
    "    \"\"\"\n",
    "    Compute the ML estimate of a probability vector based on weak labels in iz and the mixing matrix M.\n",
    "    The estimation method is based on Expectation Maximization.\n",
    "    \n",
    "    Args:\n",
    "        iz       :Observed weak labels\n",
    "        M        :Mixing matrix\n",
    "        f0       :Initial value of the ML estimate.\n",
    "        max_iter :Maximum number of iterations.\n",
    "        echo     :If 'on', output messages are shown\n",
    "\n",
    "    Returns:\n",
    "        f_ml     :Maximum likelihood estimate\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the estimate.\n",
    "    if f0 is None:\n",
    "        C = M.shape[1]   # No. of classes\n",
    "        f_ml = np.ones(C)/C\n",
    "    else:\n",
    "        f_ml = f0\n",
    "\n",
    "    # Recursive estimation\n",
    "    iterate = True\n",
    "    count = 0\n",
    "    while iterate:\n",
    "        fi = np.dot(np.diag(f_ml), M.T)[:,iz.astype(int)]\n",
    "        fi = fi / np.sum(fi, axis=0)\n",
    "        f_new = np.mean(fi, axis=1)\n",
    "        count += 1\n",
    "        iterate = np.any(f_new != f_ml) and count < max_iter\n",
    "        f_ml = np.copy(f_new)\n",
    "    \n",
    "    if echo=='on':\n",
    "        if count>= max_iter:\n",
    "            print(\"Stopped before convergence after {0} iterations\".format(max_iter))\n",
    "        else:\n",
    "            print(\"Converged in {0} iterations\".format(count))\n",
    "\n",
    "    return f_ml\n",
    "\n",
    "def computeNLL(iz, M, f):\n",
    "    \"\"\"\n",
    "    Compute the Log-Likelihood function for an estimate f.\n",
    "    \"\"\"\n",
    "    \n",
    "    I = np.eye(M.shape[0])\n",
    "    z = I[iz.astype(int)]\n",
    "    NLL = - np.dot(np.mean(z, axis=0), np.log(np.dot(M, f)))\n",
    "\n",
    "    return NLL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the EM steps monotonically decrease the NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_it = 10000\n",
    "\n",
    "I = np.eye(M.shape[0])\n",
    "\n",
    "# Compute inverted index \n",
    "z2i = dict(zip(dec_labels, range(len(dec_labels))))\n",
    "\n",
    "# Transform (decimal) weak labels into their corresponding indices in dec_labels.\n",
    "iz = np.array([z2i[zi] for zi in z])\n",
    "\n",
    "# Compute weak label proportions\n",
    "B = I[iz]\n",
    "q = np.mean(B, axis=0)\n",
    "\n",
    "# Compute ML estimate based on iz and M\n",
    "f_lim = computeML(iz, M, f, max_iter=10000, echo='on')\n",
    "\n",
    "f = None\n",
    "NLL = []\n",
    "MSE = []\n",
    "KL = []\n",
    "MSE_EM = []\n",
    "for i in range(n_it):\n",
    "    \n",
    "    f_new = computeML(iz, M, f, max_iter=1)\n",
    "    \n",
    "    if f is not None and np.all(f_new == f):\n",
    "        break\n",
    "    else:\n",
    "        f = np.copy(f_new)\n",
    "\n",
    "    # NLL.append(computeNLL(iz, M, f) + np.dot(q, np.log(q)))\n",
    "    NLL.append(computeNLL(iz, M, f) + np.dot(q, np.log(np.dot(M, f_lim))))\n",
    "    MSE.append(np.sum((f - eta)**2))\n",
    "    KL.append(- np.dot(eta, np.log(f)) + np.dot(eta, np.log(eta)))\n",
    "    MSE_EM.append(np.sum((f - f_lim)**2))\n",
    "\n",
    "print \"eta = {0}\".format(eta)\n",
    "print \"f_ml = {0}\".format(f)\n",
    "print \"f_v = {0}\".format(f_v)\n",
    "its = range(len(NLL))\n",
    "plt.loglog(its, NLL, label= \"Normalized NLL\")\n",
    "plt.loglog(its, MSE, label= \"MSE\")\n",
    "plt.loglog(its, MSE_EM, label= \"MSE_EM\")\n",
    "plt.loglog(its, KL, label= \"KL divergence\")\n",
    "plt.legend(loc='best')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylim((1e-14, plt.ylim()[1]))\n",
    "plt.show()\n",
    "print \"The final estimate is {0}\".format(f)\n",
    "print \"The true label is {0}\".format(eta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we can expect that both the *Normalized NLL* and the *MSE_EM* converge to zero, because both measures are based on the comparison with the limit value. The curves are usefull to verify that the error measures decreas monotonically.\n",
    "\n",
    "The *MSE* and the *KL divergence* do not usually converge to zero, because of the finite sample size. Yoy may check that the limit values of these error measures are smaller when the sample size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_ml = computeML(iz, M, max_iter=100000, echo='on')\n",
    "\n",
    "print \"ML estimate: {0}\".format(f_ml)\n",
    "e2 = np.sum((f_ml - eta)**2)\n",
    "print \"Square error: {0}\".format(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical analysis of the MSE.\n",
    "\n",
    "We will compute all estimates multiple times in order to compare the distribution of the MSE.\n",
    "\n",
    "First, to make sure that the WLL estimate is working properly, we plot the convergence of the estimate with the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_sim = 100\n",
    "mse = {'wll': []}\n",
    "K = 1000\n",
    "for n in range(n_sim):\n",
    " \n",
    "    if (n+1)/1*1 == n+1:\n",
    "        print '\\r Simulation {0} out of {1}'.format(str(n+1), n_sim),\n",
    "    # Generate true labels\n",
    "    iy = np.random.choice(np.arange(0, C), size=K, p=eta)\n",
    "\n",
    "    # Generate weak label indices\n",
    "    z = wlw.generateWeak(iy, M, dec_labels)\n",
    "\n",
    "    # Estimation with virtual labels (M and dec_labels are not used if vl_model=='Mproper')\n",
    "    v = wlw.computeVirtual(z, C, method=vl_model, M=M, dec_labels=dec_labels)\n",
    "        \n",
    "    f_v = np.cumsum(v, axis=0) / np.arange(1, K+1)[:,np.newaxis]\n",
    "    mse_n = np.sum((f_v - eta)**2, axis=1)\n",
    "    mse['wll'].append(mse_n)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse_mean = np.mean(mse['wll'], axis=0)\n",
    "d = mse['wll'] - mse_mean\n",
    "mse_std_u = np.sqrt(np.sum(d**2*(d >=0), axis=0)/np.sum((d >=0), axis=0))\n",
    "mse_std_d = np.sqrt(np.sum(d**2*(d <=0), axis=0)/np.sum((d <=0), axis=0))\n",
    "plt.fill_between(range(K), mse_mean - mse_std_d, mse_mean + mse_std_u,\n",
    "    alpha=0.2, edgecolor='#1B2ACC', facecolor='#089FFF',\n",
    "    linewidth=1, linestyle='solid', antialiased=True)\n",
    "plt.loglog(range(K), mse_mean)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Sample size')\n",
    "plt.ylabel('Average square error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Supervision vs partial supervision\n",
    "\n",
    "In the following we test, for a fixed sample size, the estimation of $\\boldsymbol{\\eta}$ as the average of virtual labels, in comparison with a complete supervision (i.e., as the average of the true labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_sim = 200\n",
    "mse = {'sup': [], 'wll': [], 'wml': []}\n",
    "I_C = np.eye(C)\n",
    "\n",
    "for n in range(n_sim):\n",
    " \n",
    "    if (n+1)/1*1 == n+1:\n",
    "        print '\\r Simulation {0} out of {1}'.format(str(n+1), n_sim),\n",
    "\n",
    "    # Generate true labels\n",
    "    iy = np.random.choice(np.arange(0, C), size=K, p=eta)\n",
    "    y = I_C[iy]\n",
    "\n",
    "    # Generate weak label indices\n",
    "    z = wlw.generateWeak(iy, M, dec_labels)\n",
    "    iz = np.array([z2i[zi] for zi in z])\n",
    "\n",
    "    # Supervised estimation\n",
    "    f = np.mean(y, axis=0)\n",
    "    mse['sup'].append(np.sum((f - eta)**2))\n",
    "\n",
    "    # Estimation with virtual labels (M and dec_labels are not used if vl_model=='Mproper')\n",
    "    v = wlw.computeVirtual(z, C, method=vl_model, M=M, dec_labels=dec_labels)\n",
    "\n",
    "    f_v = np.mean(v, axis=0)\n",
    "    mse['wll'].append(np.sum((f_v - eta)**2))\n",
    "\n",
    "    # Estimation with ML-EM\n",
    "    f_ml = computeML(iz, M, f0=None, max_iter=1000, echo='off')\n",
    "    mse['wml'].append(np.sum((f_ml - eta)**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following error plots shows, that, under very weak supervision, there is a significant performance degradation caused by the use of weak labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_list = ['sup', 'wll', 'wml']\n",
    "for i, tag in enumerate(tag_list):\n",
    "    plt.scatter([i + 1]*n_sim, mse[tag], c=[i]*n_sim, s=10, cmap='copper')\n",
    "\n",
    "plt.xticks(range(1, 1 + len(tag_list)), tag_list, rotation='45')\n",
    "plt.axis('tight')\n",
    "plt.ylim((0, plt.ylim()[1]))\n",
    "plt.show(block=False)\n",
    "\n",
    "plt.boxplot([mse[tag] for tag in tag_list])\n",
    "plt.xticks(range(1, 1 + len(tag_list)), tag_list, rotation='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Influence of the selection of the weak label matrix\n",
    "\n",
    "In this section we show that the selection of the virtual label matrix is relevant. Though any left-inverse of the mixing matrix provides an umbiased posterior probability estimate, the quality of the estimate for a finite sample size depends on ${\\bf M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta = [ 0.02415819  0.7185569   0.25728491]\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURABLE PARAMETERS FOR THIS SECTION\n",
    "C = 3                             # Number of classes\n",
    "\n",
    "#eta = np.array([0.5, 0.2, 0.3])   # True probability vector\n",
    "eta = np.random.dirichlet((1,1,1), 1)[0]       # True probability vector\n",
    "print(\"eta = {}\".format(eta))\n",
    "\n",
    "# Weak Label model.\n",
    "# This section is focused on the 'quasi_IPL' model. Some (maybe all) of the other available models will not work here.\n",
    "wl_model = 'quasi_IPL'  \n",
    "beta = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, for convenience, we will use a different order for the weak labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak label ordering (in decimal format): [4 2 1 3 5 6]\n",
      "Mixing matrix: \n",
      "[[ 0.11111111  0.          0.        ]\n",
      " [ 0.          0.11111111  0.        ]\n",
      " [ 0.          0.          0.11111111]\n",
      " [ 0.          0.44444444  0.44444444]\n",
      " [ 0.44444444  0.          0.44444444]\n",
      " [ 0.44444444  0.44444444  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Mixing matrix\n",
    "M = wlw.computeM(C, beta=beta, method=wl_model)\n",
    "if wl_model == 'quasi_IPL':\n",
    "    dec_labels = np.arange(0, 2**C)\n",
    "elif wl_model in ['noisy', 'random_noise']:\n",
    "    dec_labels = 2**np.arange(C-1, -1, -1)\n",
    "else:\n",
    "    raise ValueError(\"dec_labels not implemented for the given wl_model\")\n",
    "\n",
    "# Remove zero rows\n",
    "flag = np.nonzero(np.sum(M, axis=1))[0]\n",
    "M = M[flag, :]                 # This is to remove zero rows, which are not relevant\n",
    "dec_labels = dec_labels[flag]  # This is to remove zero rows, which are not relevant\n",
    "\n",
    "# Reorder rows\n",
    "dec_labels = np.array([4, 2, 1, 3, 5, 6])\n",
    "M = M[dec_labels-1]\n",
    "\n",
    "print(\"Weak label ordering (in decimal format): {}\".format(dec_labels))\n",
    "print(\"Mixing matrix: \\n{}\".format(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create two virtual label matrices, ${\\bf V}_u$ and ${\\bf V}_d$, that are left inverses of ${\\bf M}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vu = \n",
      "[[ 9.  0.  0.  0.  0.  0.]\n",
      " [ 0.  9.  0.  0.  0.  0.]\n",
      " [ 0.  0.  9.  0.  0.  0.]]\n",
      "Vd = \n",
      "[[ 0.    -0.     0.    -1.125  1.125  1.125]\n",
      " [-0.    -0.     0.     1.125 -1.125  1.125]\n",
      " [-0.     0.     0.     1.125  1.125 -1.125]]\n",
      "V = \n",
      "[[ 0.399 -0.13  -0.13  -1.043  1.075  1.075]\n",
      " [-0.13   0.399 -0.13   1.075 -1.043  1.075]\n",
      " [-0.13  -0.13   0.399  1.075  1.075 -1.043]]\n"
     ]
    }
   ],
   "source": [
    "# Get the up and down halves of the mixing matrix\n",
    "up = np.array([[1, 1, 1, 0, 0, 0]]).T\n",
    "down = np.array([[0, 0, 0, 1, 1, 1]]).T\n",
    "Mu = M*up \n",
    "Md = M*down\n",
    "pu = np.sum(Mu, axis=0, keepdims=True)\n",
    "pd = np.sum(Md, axis=0, keepdims=True)\n",
    "Mu = Mu / pu\n",
    "Md = Md / pd\n",
    "\n",
    "# Compute the virtual label matrices given by the Moore-Penrose pseudoinverses of Mu, Md and M\n",
    "Vu = np.linalg.pinv(Mu)/pu.T\n",
    "Vd = np.linalg.pinv(Md)/pd.T\n",
    "V = np.linalg.pinv(M)\n",
    "\n",
    "# Print virtual label matrices\n",
    "Vd2 = np.round(1000*Vd).astype(float)/1000    # This is just for a nicer view...\n",
    "V2 = np.round(1000*V).astype(float)/1000    # This is just for a nicer view...\n",
    "print(\"Vu = \\n{}\".format(Vu))\n",
    "print(\"Vd = \\n{}\".format(Vd2))\n",
    "print(\"V = \\n{}\".format(V2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any convex combination \n",
    "\n",
    "$${\\bf V} = w {\\bf V}_u + (1-w) {\\bf V}_d$$\n",
    "\n",
    "is also a virtual matrix. Thus, we can evaluate the behavior of the estimator as a function of $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEPCAYAAABFpK+YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/5WPgVWKkuFwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116f87550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal weight is: 0.272727272727\n",
      "The optimal mixed virtual label matrix is: \n",
      "[[ 2.455 -0.     0.    -0.818  0.818  0.818]\n",
      " [-0.     2.455  0.     0.818 -0.818  0.818]\n",
      " [-0.     0.     2.455  0.818  0.818 -0.818]]\n",
      "The optimal mixed MSE is: 1.87144229544\n",
      "The optimal virtual label matrix is: \n",
      "[[ 0.399 -0.13  -0.13  -1.043  1.075  1.075]\n",
      " [-0.13   0.399 -0.13   1.075 -1.043  1.075]\n",
      " [-0.13  -0.13   0.399  1.075  1.075 -1.043]]\n",
      "The optimal MSE is: 2.45935788797\n"
     ]
    }
   ],
   "source": [
    "# Analytical computation of the MSE\n",
    "etav = eta[:, np.newaxis]    # The true probability vector, in column form\n",
    "w = np.linspace(0, 1, num=101)\n",
    "\n",
    "p = np.dot(M, etav)\n",
    "D = np.diag(p.T[0])\n",
    "mse_w = []\n",
    "for wi in w:\n",
    "    Vi = wi * Vu + (1 - wi) * Vd\n",
    "    E = Vi - etav\n",
    "    e2 = np.trace(np.dot(np.dot(E, D), E.T))\n",
    "    mse_w.append(e2)\n",
    "\n",
    "mse_w = np.array(mse_w)\n",
    "\n",
    "# Analytical computation of the optimal weight\n",
    "Ed = etav - Vd\n",
    "delta = Vu - Vd\n",
    "wopt = (np.trace(np.dot(np.dot(Ed, D), delta.T)) /\n",
    "        np.trace(np.dot(np.dot(delta, D), delta.T)))\n",
    "\n",
    "# Optimal mixed virtual label matrix\n",
    "Vw = wopt * Vu + (1 - wopt) * Vd\n",
    "E = Vw - etav\n",
    "mse_wopt = np.dot(np.sum(E*E, axis=0), p)[0]\n",
    "\n",
    "# Optimal virtual label matrix\n",
    "E = V - etav\n",
    "mse_opt = np.dot(np.sum(E*E, axis=0), p)[0]\n",
    "\n",
    "    \n",
    "plt.plot(w, mse_w, label='Mixed VLM')\n",
    "plt.stem([wopt], [mse_wopt])\n",
    "plt.plot([0, 1], [mse_opt, mse_opt], label='Bound')\n",
    "plt.xlabel('$w$')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Virtual label matrix for the optimal weight:\n",
    "print(\"The optimal weight is: {}\".format(wopt))\n",
    "Vw2 = np.round(1000*Vw).astype(float)/1000    # This is just for a nicer view...\n",
    "print(\"The optimal mixed virtual label matrix is: \\n{}\".format(Vw2))\n",
    "print(\"The optimal mixed MSE is: {}\".format(mse_wopt))\n",
    "\n",
    "\n",
    "# Virtual label matrix for the optimal weight:\n",
    "V2 = np.round(1000*V).astype(float)/1000    # This is just for a nicer view...\n",
    "print(\"The optimal virtual label matrix is: \\n{}\".format(V2))\n",
    "print(\"The optimal MSE is: {}\".format(mse_opt))\n",
    "\n",
    "# This is to check that all virtual matrices computed here are indeed left inverses of M\n",
    "# print(np.round(1000*np.dot(Vu, M).astype(float)/1000))\n",
    "# print(np.round(1000*np.dot(Vd, M).astype(float)/1000))\n",
    "# print(np.round(1000*np.dot(Vw, M).astype(float)/1000))\n",
    "# print(np.round(1000*np.dot(V, M).astype(float)/1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to check that the analytical MSE is correct and fits the empirical estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Simulation 100 out of 100"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEPCAYAAACUb2mtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERFxncJGRERcp7ARERHX/X/PpKN+sffWLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117bfd210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The optimal weight is 0.272727272727\n",
      "The optimal MSE is: 0.000491871577593\n",
      "The average SE of the optimal Virtual Matrix is [ 0.00046146]\n"
     ]
    }
   ],
   "source": [
    "# Configurable parameters\n",
    "K = 5000      # Sample size\n",
    "n_sim = 100    # No. of simulations to average\n",
    "\n",
    "# Weak label indices\n",
    "ilabels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "I_C = np.eye(C)\n",
    "ase = 0\n",
    "ase_opt = 0\n",
    "for n in range(n_sim):\n",
    " \n",
    "    if (n+1)/1*1 == n+1:\n",
    "        print '\\r Simulation {0} out of {1}'.format(str(n+1), n_sim),\n",
    "\n",
    "    # Generate true labels\n",
    "    iy = np.random.choice(np.arange(0, C), size=K, p=eta)\n",
    "\n",
    "    # Generate weak label indices\n",
    "    iz = wlw.generateWeak(iy, M, dec_labels=ilabels)\n",
    "\n",
    "    # Up and down estimates\n",
    "    fu = np.mean(Vu[:, iz], axis=1, keepdims=True)\n",
    "    fd = np.mean(Vd[:, iz], axis=1, keepdims=True)\n",
    "    fo = np.mean(V[:, iz], axis=1, keepdims=True)\n",
    "    f = w * fu + (1 - w) * fd\n",
    "    \n",
    "    ase += np.sum((f - etav)**2, axis=0)\n",
    "    ase_opt += np.sum((fo - etav)**2, axis=0)\n",
    "\n",
    "ase = ase / n_sim\n",
    "ase_opt = ase_opt / n_sim\n",
    "\n",
    "plt.plot(w, ase, label='AvSE')\n",
    "plt.plot(w, mse_w/K, label='MSE')\n",
    "plt.plot([0, 1], [mse_opt/K, mse_opt/K], label='Bound')\n",
    "plt.plot([0, 1], [ase_opt, ase_opt], label='Bound (est.)')\n",
    "plt.stem([wopt], [mse_wopt/K])\n",
    "plt.xlabel('$w$')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"The optimal weight is {}\".format(wopt))\n",
    "print(\"The optimal MSE is: {}\".format(mse_opt/K))\n",
    "print(\"The average SE of the optimal Virtual Matrix is {}\".format(ase_opt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify if the minimun does not depend on $\\boldsymbol{\\eta}$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ed = etav - Vd\n",
    "delta = Vu - Vd\n",
    "\n",
    "for i in range(10):\n",
    "    eta = np.random.dirichlet((1,1,1), 1)[0]\n",
    "    etav = eta[:, np.newaxis]    # The true probability vector, in column form\n",
    "    D = np.diag(np.dot(M0, etav).T[0])\n",
    "    a = np.trace(np.dot(np.dot(Ed, D), delta.T))\n",
    "    b = np.trace(np.dot(np.dot(delta, D), delta.T))\n",
    "\n",
    "    print(np.diag(np.dot(np.dot(delta, D), delta.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M0 = np.random.rand(6, 3)\n",
    "M0 = M0 / np.sum(M0, axis=0)\n",
    "V = np.linalg.pinv(M0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(M0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eta = np.random.dirichlet((1, 1, 1), 1)[0]\n",
    "etav = eta[:, np.newaxis]    # The true probability vector, in column form\n",
    "\n",
    "Meta = np.dot(M0, etav)\n",
    "DMeta = np.diag(Meta.T[0]) \n",
    "A = np.dot(Meta.T, np.diag(np.dot(V.T, V)))[0]\n",
    "mse = A - np.dot(etav.T, etav)[0, 0]\n",
    "print(A)\n",
    "print(mse)\n",
    "\n",
    "print(V.T)\n",
    "r = np.diag(np.dot(V.T, V))\n",
    "print(r)\n",
    "print(np.dot(M0.T, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combination of datasets\n",
    "\n",
    "In the following experiments we explore the combination of a fully labeled dataset with a weakly labeled dataset. We show that the beharior of the estimate based on virtual label depends on the choice of the virtual label vector.\n",
    "\n",
    "The experiment demonstrates that, though different virtual label matrices can be asymptotically equivalent for the estimation of the probability vector, they show a different behavior under finite samples. \n",
    "\n",
    "### 4.1. Weighting samples\n",
    "\n",
    "In the following experiments we explore virtual matrices ${\\bf V}$ which are a combination of virtual matrices from the original datasets, i.e. they have the form ${\\bf V} = (w{\\bf V}_0, (1-w){\\bf V}_1)$ where ${\\bf V}_0$ and ${\\bf V}_1$ are virtual matrices for the original datasets. We show that the empirical mse depends on $w$, and there is an optimal choice for $w$.\n",
    "\n",
    "Note that the experiment does not explore all posible virtual matrices (i.e. all left inverses of the mixing matrix), but only those that are a composition of two fixed virtual matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_sim = 100\n",
    "K = 5000\n",
    "\n",
    "mse = []\n",
    "for n in range(n_sim):\n",
    "\n",
    "    if (n+1)/1*1 == n+1:\n",
    "        print str(n+1),\n",
    "\n",
    "    # Generate fully labeled dataset\n",
    "    iy = np.random.choice(np.arange(0, C), size=K, p=eta)\n",
    "    y = I_C[iy]\n",
    "\n",
    "    # Generate weakly labeled dataset\n",
    "    iy = np.random.choice(np.arange(0, C), size=K, p=eta)\n",
    "    z = wlw.generateWeak(iy, M, dec_labels)\n",
    "\n",
    "    # Supervised estimation\n",
    "    f = np.mean(y, axis=0)\n",
    "\n",
    "    # Estimation with virtual labels (M and dec_labels are not used if vl_model=='Mproper')\n",
    "    v = wlw.computeVirtual(z, C, method=vl_model, M=M, dec_labels=dec_labels)\n",
    "\n",
    "    f_v = np.mean(v, axis=0)\n",
    "    \n",
    "    # Combination of virtual labels. \n",
    "    # Al values of w provide consistent virtual matrices. However, the msw for a finite sample depends on w.\n",
    "    w = np.arange(0, 1, 0.001)[:, np.newaxis]\n",
    "    f_est = f * w + f_v * (1 - w)\n",
    "    mse_n = np.sum((f_est - eta)**2, axis=1)\n",
    "    mse.append(mse_n)\n",
    "\n",
    "plt.plot(w.flatten(), np.array(mse).T)\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('mse')\n",
    "plt.xlim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "mse_mean =  np.mean(np.array(mse), axis=0)\n",
    "mse_std = np.std(np.array(mse), axis=0)\n",
    "plt.plot(w.flatten(), mse_mean)\n",
    "plt.fill_between(w.flatten(), mse_mean - mse_std, mse_mean + mse_std,\n",
    "    alpha=0.2, edgecolor='#1B2ACC', facecolor='#089FFF',\n",
    "    linewidth=4, linestyle='dashdot', antialiased=True)\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('mse')\n",
    "plt.xlim([0, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.2. Optimal weight versus noise level\n",
    "\n",
    "The following experiment shows that the relation between the noise level and the optimal weight might be non-trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Ktrue = 2000       # Number of clean labels\n",
    "Kweak = 2000       # Number of weak labels       \n",
    "qTrue = float(Ktrue)/(Ktrue + Kweak)\n",
    "qWeak = float(Kweak)/(Ktrue + Kweak)\n",
    "\n",
    "n_betas = 10\n",
    "beta_set = np.linspace(0, 1-1.0/n_betas, n_betas)\n",
    "\n",
    "n_sim = 200      # Number of experiments for each value of eta.\n",
    "wmse = []\n",
    "wkld = []\n",
    "w = np.linspace(0, 1, 101)[:, np.newaxis]/ qTrue\n",
    "\n",
    "for beta in beta_set:\n",
    "\n",
    "    print \"\\rBeta = {0}\".format(beta),\n",
    "    \n",
    "    # Mixing matrix\n",
    "    M_b = wlw.computeM(C, beta=beta, method=wl_model)\n",
    "    if wl_model == 'quasi_IPL':\n",
    "        dec_labels_b = np.arange(0, 2**C)\n",
    "    elif wl_model in ['noisy', 'random_noise']:\n",
    "        dec_labels_b = 2**np.arange(C-1, -1, -1)\n",
    "    else:\n",
    "        raise ValueError(\"dec_labels not implemented for the given wl_model\")\n",
    "\n",
    "    # Compute virtual matrix (this is to compute virtual labels in a more efficient way\n",
    "    # than the current implementation of ComputeVirtual in WLweakener)\n",
    "    V = np.linalg.pinv(M_b)    \n",
    "    \n",
    "    np.random.seed(0)\n",
    "    mse = []\n",
    "    kld = []\n",
    "    for n in range(n_sim):\n",
    "\n",
    "        # Generate fully labeled dataset\n",
    "        iy = np.random.choice(np.arange(0, C), size=Ktrue, p=eta)\n",
    "        y = I_C[iy]\n",
    "\n",
    "        # Generate weakly labeled dataset\n",
    "        iy2 = np.random.choice(np.arange(0, C), size=Kweak, p=eta)\n",
    "        z2 = wlw.generateWeak(iy2, M_b, dec_labels_b)\n",
    "\n",
    "        # Compute inverted index \n",
    "        z2i = dict(zip(dec_labels_b, range(len(dec_labels_b))))\n",
    "        # Transform (decimal) weak labels into their corresponding indices in dec_labels.\n",
    "        iz2 = np.array([z2i[zi] for zi in z2])\n",
    "    \n",
    "        # Supervised estimation\n",
    "        f = np.mean(y, axis=0)\n",
    "\n",
    "        # Estimation with virtual labels\n",
    "        # v = wlw.computeVirtual(iz, C, method='Mproper', M=M)\n",
    "        v = V.T[iz2.astype(int)]\n",
    "        f_v = np.mean(v, axis=0)\n",
    "        \n",
    "        # if np.any(f_v<=0):\n",
    "        #     print \"f_v = {0}\".format(f_v)\n",
    "\n",
    "        # Weighted average\n",
    "        f_est = f * w * qTrue + f_v * (1 - w * qTrue)\n",
    "        # f_est = f * w + f_v * (1 - w)\n",
    "        mse_n = np.sum((f_est - eta)**2, axis=1)\n",
    "        mse.append(mse_n)\n",
    "        \n",
    "        kld_n = - np.dot(eta, np.log(f_est.T)) \n",
    "        kld.append(kld_n)\n",
    "\n",
    "    mse_mean =  np.mean(np.array(mse), axis=0)\n",
    "    imin = np.argmin(mse_mean)\n",
    "    wmse.append(w[imin][0])\n",
    "    \n",
    "    kld_mean =  np.mean(np.array(kld), axis=0)\n",
    "    imin = np.argmin(kld_mean)\n",
    "    wkld.append(w[imin][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(beta_set, np.array(wmse).flatten(), label=\"MSE\")\n",
    "plt.plot(beta_set, np.array(wkld).flatten(), label=\"KLD\")\n",
    "plt.xlabel('beta (i.e. from low to high noise)')\n",
    "plt.ylabel('wmin')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last experiment shows several important issues:\n",
    "\n",
    "* The optimal weights could be independent on the choice of the proper loss\n",
    "* The average of the virtual labels can be out of the probability simplex. In this respect, the optimal probability estimate should be computed with the constraint that the estimate lies inside the probability simplex. (Negative values are the cause of the anomalies in the KL divergence weighs that may appear in the above plot).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.3. A comparison between EM, virtual labels and the optimal weights\n",
    "\n",
    "The following section shows that, despite ML-EM and weak losses may lead to different results, they can show very similar performance, though the results may depend on the selection of the configurable parameters (in particular, label proportions and mixing matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ktrue = 40       # Number of clean labels\n",
    "Kweak = 40       # Number of weak labels       \n",
    "qTrue = float(Ktrue)/(Ktrue + Kweak)\n",
    "qWeak = float(Kweak)/(Ktrue + Kweak)\n",
    "\n",
    "n_betas = 20\n",
    "beta_set = np.linspace(0, 1.0-1.0/n_betas, n_betas)\n",
    "n_sim = 2000      # Number of experiments for each value of eta.\n",
    "wmse = []\n",
    "w = np.linspace(0, 1, 101)[:, np.newaxis]/ qTrue\n",
    "\n",
    "mse_w = []\n",
    "mse_v = []\n",
    "mse_ml = []\n",
    "wtest = []\n",
    "wpseudo = []\n",
    "\n",
    "for beta in beta_set:\n",
    "\n",
    "    print \"\\rBeta = {0}\".format(beta),\n",
    "    \n",
    "    # Mixing matrix\n",
    "    M_b = wlw.computeM(C, beta=beta, method=wl_model)\n",
    "    if wl_model == 'quasi_IPL':\n",
    "        dec_labels_b = np.arange(0, 2**C)\n",
    "    elif wl_model == 'noisy':\n",
    "        dec_labels_b = 2**np.arange(C-1, -1, -1)\n",
    "    else:\n",
    "        raise ValueError(\"dec_labels not implemented for the given wl_model\")\n",
    "    # Remove zero rows\n",
    "    flag = np.nonzero(np.sum(M_b, axis=1))[0]\n",
    "    M_b = M_b[flag, :]   # This is to remove zero rows, which are not relevant\n",
    "    dec_labels_b = dec_labels_b[flag]   # This is to remove zero rows, which are not relevant\n",
    "        \n",
    "    # Compute virtual matrix \n",
    "    # Note that the virtual label matrix is equivalent to the (transpose of the) matrix of virtual labels \n",
    "    # corresponding to the weak labels in dec_labels.\n",
    "    # Computing the virtual matrix here we avoid computing a pseudoinverse inside the \"for\" loop below\n",
    "    V = wlw.computeVirtual(dec_labels_b, C, method=vl_model, M=M_b, dec_labels=dec_labels_b).T\n",
    "    \n",
    "    # Compute combined mixing matrix\n",
    "    M2 = np.vstack((qTrue*np.eye(C), qWeak*M_b))\n",
    "    V2 = np.linalg.pinv(M2)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    mse_wn = []\n",
    "    mse_vn = []\n",
    "    mse_mln = []\n",
    "\n",
    "    for n in range(n_sim):\n",
    "\n",
    "        # #####################\n",
    "        # ## Dataset generation\n",
    "        \n",
    "        # Generate fully labeled dataset\n",
    "        iy = np.random.choice(np.arange(0, C), size=Ktrue, p=eta)\n",
    "        y = I_C[iy]\n",
    "\n",
    "        # Generate weakly labeled dataset\n",
    "        iy2 = np.random.choice(np.arange(0, C), size=Kweak, p=eta)\n",
    "        z = wlw.generateWeak(iy2, M_b, dec_labels_b)\n",
    "\n",
    "        # Compute inverted index \n",
    "        z2i = dict(zip(dec_labels_b, range(len(dec_labels_b))))\n",
    "        # Transform (decimal) weak labels into their corresponding indices in dec_labels.\n",
    "        iz = np.array([z2i[zi] for zi in z])\n",
    "\n",
    "        # Join datasets\n",
    "        iz2 = np.hstack((iy, iz + C))\n",
    "        \n",
    "        # #######################\n",
    "        # ## Weighted combination\n",
    "        \n",
    "        # Supervised estimation with dataset 0\n",
    "        f = np.mean(y, axis=0)\n",
    "\n",
    "        # Estimation with virtual labels and dataset 1\n",
    "        # v = wlw.computeVirtual(iz, C, method='Mproper', M=M)\n",
    "        v = V.T[iz.astype(int)]\n",
    "        \n",
    "        f_v = np.mean(v, axis=0)\n",
    "        \n",
    "        # Weighted average\n",
    "        f_est = f*w*qTrue + f_v*(1-w*qTrue)\n",
    "        mse_wn.append(np.sum((f_est - eta)**2, axis=1))\n",
    "\n",
    "        # #######################\n",
    "        # ## (pinv) M-proper loss\n",
    "        v2 = V2.T[iz2.astype(int)]\n",
    "        f_v2 = np.mean(v2, axis=0)\n",
    "        mse_vn.append(np.sum((f_v2 - eta)**2))        \n",
    "        \n",
    "        # ##############\n",
    "        # ## ML estimate\n",
    "        f_ml = computeML(iz2, M2, f0=None, max_iter=1000, echo='off')\n",
    "        mse_mln.append(np.sum((f_ml - eta)**2))\n",
    "\n",
    "    mse_mean =  np.mean(np.array(mse_wn), axis=0)\n",
    "    imin = np.argmin(mse_mean)\n",
    "    wmse.append(w[imin][0])\n",
    "    mse_w.append(np.min(mse_mean))\n",
    "\n",
    "    mse_v.append(np.mean(np.array(mse_vn), axis=0))\n",
    "    mse_ml.append(np.mean(np.array(mse_mln), axis=0))\n",
    "    \n",
    "    F11 = qWeak**2 * np.linalg.norm(np.dot(M, V), 'fro')**2\n",
    "    F10 = qWeak**2 * np.linalg.norm(M, 'fro')**2\n",
    "    F01 = qTrue**2 * np.linalg.norm(V, 'fro')**2\n",
    "    F00 = qTrue**2*C\n",
    "    w0= qTrue*C/(F00 + F10)\n",
    "    \n",
    "    wtest.append(qTrue*(F11+F01) / (qTrue**2*(F11+F01) + qWeak**2*(F00+F10)))\n",
    "    wpseudo.append((w0/qTrue-1))\n",
    "    \n",
    "print mse_w\n",
    "print mse_v\n",
    "print mse_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next figure shows the MSE obtained with ML-EM (red), virtual labels based on the left inverse of ${\\bf M}$ provided by `pinv` (blue) and virtual labels with the empirically optimal weights. The results may depend on the choice of the mixing matrix, but in general there is a good adjustment between ML and virtual labels. Also, there seem to exist some optimal weight providing and MSE equivalent to ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(beta_set, np.array(mse_w).flatten(), label=\"WLL_w\")\n",
    "plt.plot(beta_set, np.array(mse_v).flatten(), label=\"WLL\")\n",
    "plt.plot(beta_set, np.array(mse_ml).flatten(), label=\"ML-EM\")\n",
    "plt.xlabel('beta (from low to high noise)')\n",
    "plt.ylabel('mse')\n",
    "# plt.ylim((0, np.max(mse_w)+0.001))\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print mse_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the draft paper I have derived some equations to compute the optimal weights of a combination of two given virtual matrices.\n",
    "These weights are computed below, for different values of beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_betas = 20\n",
    "beta_set = np.linspace(0, 1-1.0/n_betas, n_betas)\n",
    "wtest = []\n",
    "wpseudo = []\n",
    "\n",
    "for beta in beta_set:\n",
    "\n",
    "    print \"\\rBeta = {0}\".format(beta),\n",
    "\n",
    "    # Mixing matrix\n",
    "    M = wlw.computeM(C, beta=beta, method=wl_model)\n",
    "\n",
    "    # Compute virtual matrix (this is to compute virtual labels in a more efficient way\n",
    "    # than the current implementation of ComputeVirtual in WLweakener)\n",
    "    V = np.linalg.pinv(M)\n",
    "\n",
    "    # Compute combined mixing matrix\n",
    "    M2 = np.vstack((qTrue*np.eye(C), qWeak*M))\n",
    "    V2 = np.linalg.pinv(M2)\n",
    "\n",
    "    F11 = qWeak**2 * np.linalg.norm(np.dot(M, V), 'fro')**2\n",
    "    F10 = qWeak**2 * np.linalg.norm(M, 'fro')**2\n",
    "    F01 = qTrue**2 * np.linalg.norm(V, 'fro')**2\n",
    "    F00 = qTrue**2*C\n",
    "    w0= qTrue*C/(F00 + F10)\n",
    "    \n",
    "    wtest.append(qTrue*(F11 + F01) / (qTrue**2*(F11+F01) + qWeak**2*(F00+F10)))\n",
    "    wpseudo.append(w0/qTrue-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next figure shows the difference between the empirically optimal weights (blue), the prediction (green) and a slight modification of the prediction that seems to improve the adjustment between predicted and optimal weights in a significant manner. No conclusions can be made yet, because there could be errors in the derived equations (to be checked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(beta_set, np.array(wmse).flatten(), label=\"Empirical\")\n",
    "plt.plot(beta_set, np.array(wtest).flatten(), label=\"Prediction\")\n",
    "plt.plot(beta_set, np.array(wpseudo).flatten(), label=\"Pseudo-pred\")\n",
    "plt.xlabel('beta (from low to high noise)')\n",
    "plt.ylabel('wmin')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "print np.array(wmse).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta_set = np.linspace(0, 1, 21)\n",
    "print beta_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
